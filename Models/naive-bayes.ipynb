{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier\n",
    "\n",
    "Sticking to multinomial now, as I've seen it used much more\n",
    "\n",
    "Here's a really in-depth resource explaining the entire intuition about NB sentiment analysis: https://web.stanford.edu/~jurafsky/slp3/4.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"comments_preproc.csv\").sample(n=200000, random_state=1)\n",
    "reviews.dropna(subset=[\"cleanedComment\"], inplace=True)\n",
    "reviews.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    147956\n",
       "0     52037\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reviews[\"sentiment\"] = reviews[\"clarityRating\"].apply(lambda x: 1 if x > 3 else 0 if x == 3 else -1)\n",
    "reviews[\"sentiment\"] = reviews[\"clarityRating\"].apply(lambda x: 1 if x > 2.5 else 0)\n",
    "reviews[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_proper = [x for x in reviews[\"cleanedComment\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNGrams(n=2):\n",
    "    all_words = \" \".join(comments_proper)\n",
    "    all_words = all_words.split()               # get full list of unigrams in data\n",
    "    \n",
    "    ngs = nltk.ngrams(all_words, n=n)           # find ngrams of data\n",
    "    \n",
    "    ngram_vectors = []\n",
    "    for item in ngs:\n",
    "        ngram_vectors.append(\" \".join(item))    # create vectors for each ngram\n",
    "    return ngram_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more info on eval metrics used, \n",
    "* Confusion Matrix: https://towardsdatascience.com/understanding-the-confusion-matrix-from-scikit-learn-c51d88929c79\n",
    "* Precision + Recall: https://en.m.wikipedia.org/wiki/Precision_and_recall\n",
    "* F1 Score: https://www.educative.io/answers/what-is-the-f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalPerformance(y_pred, y_test, mode=\"weighted\"):\n",
    "    conf_m = confusion_matrix(y_test, y_pred)\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy Score: \" + str(acc_score * 100) + \"\\n\")\n",
    "    print(\"Confusion Matrix: \")\n",
    "    print(conf_m)\n",
    "    print()\n",
    "\n",
    "    precision = precision_score(y_test, y_pred, average=mode)\n",
    "    recall = recall_score(y_test, y_pred, average=mode)\n",
    "\n",
    "    print(\"Precision: {0}\".format(precision * 100))\n",
    "    print(\"Recall: {0}\\n\".format(recall * 100))\n",
    "\n",
    "    f1 = f1_score(y_test, y_pred, average=mode)\n",
    "    print(\"F1 Score: {0}\".format(f1 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizeFeatures():\n",
    "    feat = range(4750, 5250, 100)\n",
    "    accs = []\n",
    "    for i in feat:\n",
    "        cv = CountVectorizer(max_features=i, ngram_range=(1,1), binary=True)\n",
    "        X = cv.fit_transform(comments_proper).toarray()       # get list of features (comments)\n",
    "        y = pd.get_dummies(reviews.loc[:, [\"sentiment\"]])     # isolate sentiments\n",
    "        y = y.loc[:, \"sentiment\"]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)        # split into training and testing subsets\n",
    "        model = MultinomialNB().fit(X_train, y_train)           # create and fit model, use it to predict outcomes on test set\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        accs.append(acc)\n",
    "    \n",
    "    plt.scatter(feat, accs)\n",
    "    plt.figure(figsize=(25,5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizeNgrams():\n",
    "    feat = range(1, 5)\n",
    "    accs = []\n",
    "    for i in feat:\n",
    "        cv = CountVectorizer(max_features=3000, ngram_range=(i,i), binary=True)\n",
    "        X = cv.fit_transform(comments_proper).toarray()       # get list of features (comments)\n",
    "        y = pd.get_dummies(reviews.loc[:, [\"sentiment\"]])     # isolate sentiments\n",
    "        y = y.loc[:, \"sentiment\"]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)        # split into training and testing subsets\n",
    "        model = MultinomialNB().fit(X_train, y_train)           # create and fit model, use it to predict outcomes on test set\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        accs.append(acc)\n",
    "    \n",
    "    plt.scatter(feat, accs)\n",
    "    plt.figure(figsize=(25,5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizeFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizeNgrams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General function\n",
    "\n",
    "Takes ngram range as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runNBModel(ngr=(1,1)):\n",
    "    cv = CountVectorizer(max_features=4000, ngram_range=ngr, binary=True)\n",
    "    X = cv.fit_transform(comments_proper).toarray()       # get list of features (comments)\n",
    "\n",
    "    y = pd.get_dummies(reviews.loc[:, [\"sentiment\"]])     # isolate sentiments\n",
    "    y = y.loc[:, \"sentiment\"]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)        # split into training and testing subsets\n",
    "    \n",
    "    model = MultinomialNB().fit(X_train, y_train)           # create and fit model, use it to predict outcomes on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    evalPerformance(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start trying out Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 85.54213855346383\n",
      "\n",
      "Confusion Matrix: \n",
      "[[ 7616  2792]\n",
      " [ 2991 26600]]\n",
      "\n",
      "Precision: 85.63516928044189\n",
      "Recall: 85.54213855346383\n",
      "\n",
      "F1 Score: 85.58620302924409\n"
     ]
    }
   ],
   "source": [
    "# unigram based\n",
    "runNBModel(ngr=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 83.73709342733568\n",
      "\n",
      "Confusion Matrix: \n",
      "[[ 6865  3543]\n",
      " [ 2962 26629]]\n",
      "\n",
      "Precision: 83.46984458709136\n",
      "Recall: 83.73709342733568\n",
      "\n",
      "F1 Score: 83.58266987248302\n"
     ]
    }
   ],
   "source": [
    "# bigram based\n",
    "runNBModel(ngr=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 85.8046451161279\n",
      "\n",
      "Confusion Matrix: \n",
      "[[ 8362  2046]\n",
      " [ 3632 25959]]\n",
      "\n",
      "Precision: 86.71566507545359\n",
      "Recall: 85.8046451161279\n",
      "\n",
      "F1 Score: 86.11169501271068\n"
     ]
    }
   ],
   "source": [
    "# unigram + bigram\n",
    "runNBModel(ngr=(1,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed8ce2715666cca47edfe3dc40f16f58fc193b5a7b06958e2ae06c97afdf9990"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
