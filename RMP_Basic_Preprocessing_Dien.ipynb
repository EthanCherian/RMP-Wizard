{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RMP Basic Preprocessing Dien",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n",
        "Basic steps, because there are domain specific problems to account for later"
      ],
      "metadata": {
        "id": "gQV3OxqpZG_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import spacy\n",
        "import pkg_resources\n",
        "from spacy.lang.en.stop_words import STOP_WORDS"
      ],
      "metadata": {
        "id": "k7nhxQ8_ZI86"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install symspellpy\n",
        "\n",
        "from spacy.cli import download\n",
        "download('en_core_web_md')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jDqCGD3s7Ku",
        "outputId": "c0eee8f1-4620-4cdc-8b50-137eded4c38a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: symspellpy in /usr/local/lib/python3.7/dist-packages (6.7.6)\n",
            "Requirement already satisfied: editdistpy>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from symspellpy) (0.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from symspellpy import SymSpell, Verbosity"
      ],
      "metadata": {
        "id": "MnPL-zIAs8Ml"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMSveQ7KZ2R_",
        "outputId": "ba058e82-ed9c-42a5-e76b-b8a78fff9145"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = pd.read_csv(\"/content/drive/MyDrive/RMP/scraped_comments.csv\").sample(n=200000, random_state=1)"
      ],
      "metadata": {
        "id": "yLicijJQZLzr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Duplicates and Nulls"
      ],
      "metadata": {
        "id": "tGS9yvAMag7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews.drop_duplicates(subset=\"comment_id\", keep=\"first\", inplace=True)\n",
        "\n",
        "# drop rows containing only \"No Comments\" (default value assigned by RMP to a review that didn't enter a comment)\n",
        "reviews = reviews[reviews[\"comment\"] != \"No Comments\"]\n",
        "# drop rows containing NaN comment\n",
        "reviews.dropna(subset=[\"comment\"], inplace=True)\n",
        "\n",
        "reviews.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLUgn5sbZ_1g",
        "outputId": "e67db24b-b006-4c59-8ba9-ddaecc6dce87"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py:311: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  return func(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing Urls, Phone Numbers, and Emails"
      ],
      "metadata": {
        "id": "yDFaJC7Fe4nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_urls(text):\n",
        "    return re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
        "\n",
        "def remove_phones(text):\n",
        "    return re.sub(r'\\d{3}-\\d{3}-\\d{4}', ' ', text)\n",
        "\n",
        "def remove_emails(text):\n",
        "    return re.sub(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', ' ', text)\n",
        "\n",
        "print(remove_urls('Hey! Check out this link: www.somelink.com'))\n",
        "print(remove_phones(\"Hey! Check out this phone number: 742-457-0417\"))\n",
        "print(remove_emails(\"Hey! Check out this email address: nooneuses@yahoo.com\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzZSR-eXe9Cr",
        "outputId": "5669dbdb-5a9c-42c2-b49f-f2ce9a38c918"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey! Check out this link:  \n",
            "Hey! Check out this phone number:  \n",
            "Hey! Check out this email address:  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Html Artifacts"
      ],
      "metadata": {
        "id": "ZDTjxLjqnu-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Convert html entites of quotes -> \"'\" to normalize\n",
        "def remove_html_entities(text):\n",
        "  text = re.sub('&[0-9a-zA-Z#]+;', ' ', text)\n",
        "  return re.sub('&#63;?', '', text)\n",
        "\n",
        "def remove_html_tags(text):\n",
        "  return re.sub('<.{1,6}?>', ' ', text)\n",
        "\n",
        "text = \"This professor is such an easy &quot;A&quot;, why are y'all struggling &#63;&#63;&#63 </div>\"\n",
        "print(remove_html_entities(text))\n",
        "print(remove_html_tags(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEmTjY8mnzaQ",
        "outputId": "4872e3ac-0b19-4748-e29b-0ae883a04d7b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This professor is such an easy  A , why are y'all struggling    </div>\n",
            "This professor is such an easy &quot;A&quot;, why are y'all struggling &#63;&#63;&#63  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Emoticon Conversion to Words"
      ],
      "metadata": {
        "id": "BxBEJJO9a-Jo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Emoticon Mapping\n",
        "EMOTICONS = {\n",
        "    u\":‑)\":\"Happy face or smiley\",\n",
        "    u\":-))\":\"Very Happy face or smiley\",\n",
        "    u\":-)))\":\"Very very Happy face or smiley\",\n",
        "    u\":)\":\"Happy face or smiley\",\n",
        "    u\":))\":\"Very Happy face or smiley\",\n",
        "    u\":)))\":\"Very very Happy face or smiley\",\n",
        "    u\":-]\":\"Happy face or smiley\",\n",
        "    u\":]\":\"Happy face or smiley\",\n",
        "    u\":-3\":\"Happy face smiley\",\n",
        "    u\":3\":\"Happy face smiley\",\n",
        "    u\":->\":\"Happy face smiley\",\n",
        "    u\":>\":\"Happy face smiley\",\n",
        "    u\"8-)\":\"Happy face smiley\",\n",
        "    u\":o)\":\"Happy face smiley\",\n",
        "    u\":-}\":\"Happy face smiley\",\n",
        "    u\":}\":\"Happy face smiley\",\n",
        "    u\":-)\":\"Happy face smiley\",\n",
        "    u\":c)\":\"Happy face smiley\",\n",
        "    u\":^)\":\"Happy face smiley\",\n",
        "    u\"=]\":\"Happy face smiley\",\n",
        "    u\"=)\":\"Happy face smiley\",\n",
        "    u\":‑D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\":D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"8‑D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"8D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"X‑D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"XD\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"=D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"=3\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"B^D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\":-))\":\"Very happy\",\n",
        "    u\":-(\":\"Frown, sad, angry or pouting\",\n",
        "    u\":‑(\":\"Frown, sad, angry or pouting\",\n",
        "    u\":(\":\"Frown, sad, angry or pouting\",\n",
        "    u\":‑c\":\"Frown, sad, angry or pouting\",\n",
        "    u\":c\":\"Frown, sad, angry or pouting\",\n",
        "    u\":‑<\":\"Frown, sad, angry or pouting\",\n",
        "    u\":<\":\"Frown, sad, angry or pouting\",\n",
        "    u\":‑[\":\"Frown, sad, angry or pouting\",\n",
        "    u\":[\":\"Frown, sad, angry or pouting\",\n",
        "    u\":-||\":\"Frown, sad, angry or pouting\",\n",
        "    u\">:[\":\"Frown, sad, angry or pouting\",\n",
        "    u\":{\":\"Frown, sad, angry or pouting\",\n",
        "    u\":@\":\"Frown, sad, angry or pouting\",\n",
        "    u\">:(\":\"Frown, sad, angry or pouting\",\n",
        "    u\":'‑(\":\"Crying\",\n",
        "    u\":'(\":\"Crying\",\n",
        "    u\":'‑)\":\"Tears of happiness\",\n",
        "    u\":')\":\"Tears of happiness\",\n",
        "    u\"D‑':\":\"Horror\",\n",
        "    u\"D:<\":\"Disgust\",\n",
        "    u\"D:\":\"Sadness\",\n",
        "    u\"D8\":\"Great dismay\",\n",
        "    u\"D;\":\"Great dismay\",\n",
        "    u\"D=\":\"Great dismay\",\n",
        "    u\"DX\":\"Great dismay\",\n",
        "    u\":‑O\":\"Surprise\",\n",
        "    u\":O\":\"Surprise\",\n",
        "    u\":‑o\":\"Surprise\",\n",
        "    u\":o\":\"Surprise\",\n",
        "    u\":-0\":\"Shock\",\n",
        "    u\"8‑0\":\"Yawn\",\n",
        "    u\">:O\":\"Yawn\",\n",
        "    u\":-*\":\"Kiss\",\n",
        "    u\":*\":\"Kiss\",\n",
        "    u\":X\":\"Kiss\",\n",
        "    u\";‑)\":\"Wink or smirk\",\n",
        "    u\";)\":\"Wink or smirk\",\n",
        "    u\"*-)\":\"Wink or smirk\",\n",
        "    u\"*)\":\"Wink or smirk\",\n",
        "    u\";‑]\":\"Wink or smirk\",\n",
        "    u\";]\":\"Wink or smirk\",\n",
        "    u\";^)\":\"Wink or smirk\",\n",
        "    u\":‑,\":\"Wink or smirk\",\n",
        "    u\";D\":\"Wink or smirk\",\n",
        "    u\":‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"X‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":‑Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":‑/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\">:[(\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\">:/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":[(\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\"=/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\"=[(\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":‑|\":\"Straight face\",\n",
        "    u\":|\":\"Straight face\",\n",
        "    u\":$\":\"Embarrassed or blushing\",\n",
        "    u\":‑x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":‑#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":‑&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\"O:‑)\":\"Angel, saint or innocent\",\n",
        "    u\"O:)\":\"Angel, saint or innocent\",\n",
        "    u\"0:‑3\":\"Angel, saint or innocent\",\n",
        "    u\"0:3\":\"Angel, saint or innocent\",\n",
        "    u\"0:‑)\":\"Angel, saint or innocent\",\n",
        "    u\"0:)\":\"Angel, saint or innocent\",\n",
        "    u\":‑b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"0;^)\":\"Angel, saint or innocent\",\n",
        "    u\">:‑)\":\"Evil or devilish\",\n",
        "    u\">:)\":\"Evil or devilish\",\n",
        "    u\"}:‑)\":\"Evil or devilish\",\n",
        "    u\"}:)\":\"Evil or devilish\",\n",
        "    u\"3:‑)\":\"Evil or devilish\",\n",
        "    u\"3:)\":\"Evil or devilish\",\n",
        "    u\">;)\":\"Evil or devilish\",\n",
        "    u\"|;‑)\":\"Cool\",\n",
        "    u\"|‑O\":\"Bored\",\n",
        "    u\":‑J\":\"Tongue-in-cheek\",\n",
        "    u\"#‑)\":\"Party all night\",\n",
        "    u\"%‑)\":\"Drunk or confused\",\n",
        "    u\"%)\":\"Drunk or confused\",\n",
        "    u\":-###..\":\"Being sick\",\n",
        "    u\":###..\":\"Being sick\",\n",
        "    u\"<:‑|\":\"Dump\",\n",
        "    u\"(>_<)\":\"Troubled\",\n",
        "    u\"(>_<)>\":\"Troubled\",\n",
        "    u\"(';')\":\"Baby\",\n",
        "    u\"(^^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "    u\"(^_^;)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "    u\"(-_-;)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "    u\"(~_~;) (・.・;)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "    u\"(-_-)zzz\":\"Sleeping\",\n",
        "    u\"(^_-)\":\"Wink\",\n",
        "    u\"((+_+))\":\"Confused\",\n",
        "    u\"(+o+)\":\"Confused\",\n",
        "    u\"(o|o)\":\"Ultraman\",\n",
        "    u\"^_^\":\"Joyful\",\n",
        "    u\"(^_^)/\":\"Joyful\",\n",
        "    u\"(^O^)／\":\"Joyful\",\n",
        "    u\"(^o^)／\":\"Joyful\",\n",
        "    u\"(__)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"_(._.)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"<(_ _)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"<m(__)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"m(__)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"m(_ _)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"('_')\":\"Sad or Crying\",\n",
        "    u\"(/_;)\":\"Sad or Crying\",\n",
        "    u\"(T_T) (;_;)\":\"Sad or Crying\",\n",
        "    u\"(;_;\":\"Sad of Crying\",\n",
        "    u\"(;_:)\":\"Sad or Crying\",\n",
        "    u\"(;O;)\":\"Sad or Crying\",\n",
        "    u\"(:_;)\":\"Sad or Crying\",\n",
        "    u\"(ToT)\":\"Sad or Crying\",\n",
        "    u\";_;\":\"Sad or Crying\",\n",
        "    u\";-;\":\"Sad or Crying\",\n",
        "    u\";n;\":\"Sad or Crying\",\n",
        "    u\"Q.Q\":\"Sad or Crying\",\n",
        "    u\"T.T\":\"Sad or Crying\",\n",
        "    u\"Q_Q\":\"Sad or Crying\",\n",
        "    u\"(-.-)\":\"Shame\",\n",
        "    u\"(-_-)\":\"Shame\",\n",
        "    u\"(一一)\":\"Shame\",\n",
        "    u\"(；一_一)\":\"Shame\",\n",
        "    u\"(=_=)\":\"Tired\",\n",
        "    u\"(=^·^=)\":\"cat\",\n",
        "    u\"(=^··^=)\":\"cat\",\n",
        "    u\"=_^= \":\"cat\",\n",
        "    u\"(..)\":\"Looking down\",\n",
        "    u\"(._.)\":\"Looking down\",\n",
        "    u\"^m^\":\"Giggling with hand covering mouth\",\n",
        "    u\"(・・?\":\"Confusion\",\n",
        "    u\"(?_?)\":\"Confusion\",\n",
        "    u\">^_^<\":\"Normal Laugh\",\n",
        "    u\"<^!^>\":\"Normal Laugh\",\n",
        "    u\"^/^\":\"Normal Laugh\",\n",
        "    u\"（*^_^*）\" :\"Normal Laugh\",\n",
        "    u\"(^<^) (^.^)\":\"Normal Laugh\",\n",
        "    u\"(^^)\":\"Normal Laugh\",\n",
        "    u\"(^.^)\":\"Normal Laugh\",\n",
        "    u\"(^_^.)\":\"Normal Laugh\",\n",
        "    u\"(^_^)\":\"Normal Laugh\",\n",
        "    u\"(^^)\":\"Normal Laugh\",\n",
        "    u\"(^J^)\":\"Normal Laugh\",\n",
        "    u\"(*^.^*)\":\"Normal Laugh\",\n",
        "    u\"(^—^）\":\"Normal Laugh\",\n",
        "    u\"(#^.^#)\":\"Normal Laugh\",\n",
        "    u\"（^—^）\":\"Waving\",\n",
        "    u\"(;_;)/~~~\":\"Waving\",\n",
        "    u\"(^.^)/~~~\":\"Waving\",\n",
        "    u\"(-_-)/~~~ ($··)/~~~\":\"Waving\",\n",
        "    u\"(T_T)/~~~\":\"Waving\",\n",
        "    u\"(ToT)/~~~\":\"Waving\",\n",
        "    u\"(*^0^*)\":\"Excited\",\n",
        "    u\"(*_*)\":\"Amazed\",\n",
        "    u\"(*_*;\":\"Amazed\",\n",
        "    u\"(+_+) (@_@)\":\"Amazed\",\n",
        "    u\"(*^^)v\":\"Laughing,Cheerful\",\n",
        "    u\"(^_^)v\":\"Laughing,Cheerful\",\n",
        "    u\"((d[-_-]b))\":\"Headphones,Listening to music\",\n",
        "    u'(-\"-)':\"Worried\",\n",
        "    u\"(ーー;)\":\"Worried\",\n",
        "    u\"(^0_0^)\":\"Eyeglasses\",\n",
        "    u\"(＾ｖ＾)\":\"Happy\",\n",
        "    u\"(＾ｕ＾)\":\"Happy\",\n",
        "    u\"(^)o(^)\":\"Happy\",\n",
        "    u\"(^O^)\":\"Happy\",\n",
        "    u\"(^o^)\":\"Happy\",\n",
        "    u\")^o^(\":\"Happy\",\n",
        "    u\":O o_O\":\"Surprised\",\n",
        "    u\"o_0\":\"Surprised\",\n",
        "    u\"o.O\":\"Surpised\",\n",
        "    u\"(o.o)\":\"Surprised\",\n",
        "    u\"(*￣m￣)\":\"Dissatisfied\",\n",
        "}\n",
        "\n",
        "for emote, val in EMOTICONS.items():\n",
        "  EMOTICONS[emote] = val.lower().replace(',', ' ').replace(' ', '_')"
      ],
      "metadata": {
        "id": "Rf2Q8b2TbAsr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_emoticons(text):\n",
        "  return EMOTICONS.get(text, text)\n",
        "  \n",
        "text = \"Hello :-) :-)\"\n",
        "text_split = text.split()\n",
        "for i, txt in enumerate(text_split):\n",
        "  text_split[i] = convert_emoticons(txt)\n",
        "print(' '.join(text_split))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-jJdrc-bpp9",
        "outputId": "69bb3626-42f8-4954-8055-3fdebd42b98d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello happy_face_smiley happy_face_smiley\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contractions"
      ],
      "metadata": {
        "id": "h9wPsJ3ghEBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Contraction Mapping\n",
        "contraction_mapping = {\n",
        "    \"ain't\": \"is not\", \n",
        "    \"aren't\": \"are not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"'cause\": \"because\", \n",
        "    \"could've\": \"could have\", \n",
        "    \"couldn't\": \"could not\", \n",
        "    \"didn't\": \"did not\",  \n",
        "    \"doesn't\": \"does not\", \n",
        "    \"don't\": \"do not\", \n",
        "    \"hadn't\": \"had not\", \n",
        "    \"hasn't\": \"has not\", \n",
        "    \"haven't\": \"have not\", \n",
        "    \"he'd\": \"he would\",\n",
        "    \"he'll\": \"he will\", \n",
        "    \"he's\": \"he is\", \n",
        "    \"how'd\": \"how did\", \n",
        "    \"how'd'y\": \"how do you\", \n",
        "    \"how'll\": \"how will\", \n",
        "    \"how's\": \"how is\",  \n",
        "    \"I'd\": \"I would\", \n",
        "    \"I'd've\": \"I would have\", \n",
        "    \"I'll\": \"I will\", \n",
        "    \"I'll've\": \"I will have\",\n",
        "    \"I'm\": \"I am\", \n",
        "    \"I've\": \"I have\", \n",
        "    \"i'd\": \"i would\", \n",
        "    \"i'd've\": \"i would have\", \n",
        "    \"i'll\": \"i will\",  \n",
        "    \"i'll've\": \"i will have\",\n",
        "    \"i'm\": \"i am\", \n",
        "    \"i've\": \"i have\", \n",
        "    \"isn't\": \"is not\", \n",
        "    \"it'd\": \"it would\", \n",
        "    \"it'd've\": \"it would have\", \n",
        "    \"it'll\": \"it will\", \n",
        "    \"it'll've\": \"it will have\",\n",
        "    \"it's\": \"it is\", \n",
        "    \"let's\": \"let us\", \n",
        "    \"ma'am\": \"madam\", \n",
        "    \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VgcFh25YhG5U"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_contraction(text): # Before expanding contraction, might want to clean of symbols that are not '\n",
        "  return contraction_mapping.get(text, text)\n",
        "\n",
        "text = \"You're a pig and I should've slayed you, grrr\"\n",
        "text_split = text.split()\n",
        "for i, txt in enumerate(text_split):\n",
        "  text_split[i] = expand_contraction(txt.lower())\n",
        "print(' '.join(text_split))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl1TXvlvmjdx",
        "outputId": "42aa8b2d-e37d-4d91-d376-db3d18d5a08a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you are a pig and i should have slayed you, grrr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spellchecker"
      ],
      "metadata": {
        "id": "eWtsBJUGtPK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "dictionary_path = pkg_resources.resource_filename(\n",
        "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\"\n",
        ")\n",
        "# term_index is the column of the term and count_index is the\n",
        "# column of the term frequency\n",
        "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "\n",
        "# lookup suggestions for single-word input strings\n",
        "input_term = \"memebers\"  # misspelling of \"members\"\n",
        "\n",
        "# Verbosity.TOP gets the best suggestion\n",
        "suggestion = sym_spell.lookup(input_term, Verbosity.TOP, max_edit_distance=2)\n",
        "print(suggestion[0], len(suggestion))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKD7XvKttROM",
        "outputId": "c6130795-c336-44f7-ea71-c798df92b265"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "members, 1, 226656153 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline"
      ],
      "metadata": {
        "id": "x8c1-4PfqaCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments_proper = []\n",
        "spellchecked_comments = []\n",
        "cnt = 0 # to keep track of progress\n",
        "for comment in reviews['comment']:\n",
        "  cnt += 1\n",
        "  if cnt % 1000 == 0:\n",
        "    print(cnt)\n",
        "  comment = remove_urls(comment)\n",
        "  comment = remove_phones(comment)\n",
        "  comment = remove_emails(comment)\n",
        "  comment = remove_html_entities(comment)\n",
        "  comment = remove_html_tags(comment)\n",
        "\n",
        "  comment_split = comment.split()\n",
        "  for i, text in enumerate(comment_split):\n",
        "    text = convert_emoticons(text)\n",
        "    text = text.lower()\n",
        "    text = expand_contraction(text)\n",
        "    comment_split[i] = text\n",
        "  comment = ' '.join(comment_split)\n",
        "\n",
        "  comment = re.sub('[^a-zA-Z\\s_]+', ' ', comment)              # remove characters that are not alphabetic, space, or underscore\n",
        "  comment = re.sub(r'(.)\\1\\1+', '\\g<1>', comment)               # replace any three characters sequence with one\n",
        "  comment = re.sub('\\s+', ' ', comment)\n",
        "  \n",
        "  comments_proper.append(comment)\n",
        "  spellchecked_comments.append(' '.join(sym_spell.lookup(word, Verbosity.TOP, max_edit_distance=2, include_unknown=True)[0].term for word in comment.split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hI6lvtrgqc0i",
        "outputId": "6d933df1-4f37-4550-9e66-a1a2b711b8e7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n",
            "31000\n",
            "32000\n",
            "33000\n",
            "34000\n",
            "35000\n",
            "36000\n",
            "37000\n",
            "38000\n",
            "39000\n",
            "40000\n",
            "41000\n",
            "42000\n",
            "43000\n",
            "44000\n",
            "45000\n",
            "46000\n",
            "47000\n",
            "48000\n",
            "49000\n",
            "50000\n",
            "51000\n",
            "52000\n",
            "53000\n",
            "54000\n",
            "55000\n",
            "56000\n",
            "57000\n",
            "58000\n",
            "59000\n",
            "60000\n",
            "61000\n",
            "62000\n",
            "63000\n",
            "64000\n",
            "65000\n",
            "66000\n",
            "67000\n",
            "68000\n",
            "69000\n",
            "70000\n",
            "71000\n",
            "72000\n",
            "73000\n",
            "74000\n",
            "75000\n",
            "76000\n",
            "77000\n",
            "78000\n",
            "79000\n",
            "80000\n",
            "81000\n",
            "82000\n",
            "83000\n",
            "84000\n",
            "85000\n",
            "86000\n",
            "87000\n",
            "88000\n",
            "89000\n",
            "90000\n",
            "91000\n",
            "92000\n",
            "93000\n",
            "94000\n",
            "95000\n",
            "96000\n",
            "97000\n",
            "98000\n",
            "99000\n",
            "100000\n",
            "101000\n",
            "102000\n",
            "103000\n",
            "104000\n",
            "105000\n",
            "106000\n",
            "107000\n",
            "108000\n",
            "109000\n",
            "110000\n",
            "111000\n",
            "112000\n",
            "113000\n",
            "114000\n",
            "115000\n",
            "116000\n",
            "117000\n",
            "118000\n",
            "119000\n",
            "120000\n",
            "121000\n",
            "122000\n",
            "123000\n",
            "124000\n",
            "125000\n",
            "126000\n",
            "127000\n",
            "128000\n",
            "129000\n",
            "130000\n",
            "131000\n",
            "132000\n",
            "133000\n",
            "134000\n",
            "135000\n",
            "136000\n",
            "137000\n",
            "138000\n",
            "139000\n",
            "140000\n",
            "141000\n",
            "142000\n",
            "143000\n",
            "144000\n",
            "145000\n",
            "146000\n",
            "147000\n",
            "148000\n",
            "149000\n",
            "150000\n",
            "151000\n",
            "152000\n",
            "153000\n",
            "154000\n",
            "155000\n",
            "156000\n",
            "157000\n",
            "158000\n",
            "159000\n",
            "160000\n",
            "161000\n",
            "162000\n",
            "163000\n",
            "164000\n",
            "165000\n",
            "166000\n",
            "167000\n",
            "168000\n",
            "169000\n",
            "170000\n",
            "171000\n",
            "172000\n",
            "173000\n",
            "174000\n",
            "175000\n",
            "176000\n",
            "177000\n",
            "178000\n",
            "179000\n",
            "180000\n",
            "181000\n",
            "182000\n",
            "183000\n",
            "184000\n",
            "185000\n",
            "186000\n",
            "187000\n",
            "188000\n",
            "189000\n",
            "190000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for comment, spellcheck_comment in zip(comments_proper, spellchecked_comments):\n",
        "  print(comment)\n",
        "  print(spellcheck_comment)\n",
        "  print('\\n')\n",
        "  i += 1\n",
        "  if i == 10:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEQYPIoJsS7_",
        "outputId": "0c97e69a-a9ac-438d-de4c-7f75d94252cf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keith is the best not afraid to give his honest opinion when grading papers or choosing class discussions super laid back and funny write the papers do the work and you are good \n",
            "keith is the best not afraid to give his honest opinion when grading papers or choosing class discussions super laid back and funny write the papers do the work and you are good\n",
            "\n",
            "\n",
            "lankau was great for and he makes class fun and he is hilarious be prepared to study he is there for help but he is not going to make it an easy a you definitely have to work for your grade but he makes it worth your while just listening to him lecture \n",
            "lanka was great for and he makes class fun and he is hilarious be prepared to study he is there for help but he is not going to make it an easy a you definitely have to work for your grade but he makes it worth your while just listening to him lecture\n",
            "\n",
            "\n",
            "i hated every second of my life that i had to sit through his class he is a very intelligent man but has no idea how to teach \n",
            "i hated every second of my life that i had to sit through his class he is a very intelligent man but has no idea how to teach\n",
            "\n",
            "\n",
            "beware plagiarists \n",
            "beware plagiarist\n",
            "\n",
            "\n",
            "crazy man no grading system you dont know your grade tell student access is available he expects you to know so much when he cant teach at all take lehman over him shes alot better and hoter lol\n",
            "crazy man no grading system you done know your grade tell student access is available he expects you to know so much when he cant teach at all take lehman over him shes lot better and hotel low\n",
            "\n",
            "\n",
            "this teacher is the worst i have ever taken in my whole entire schooling career he is rude belittles students and does not teach what the test is over he puts things on the test that he did not talk about in class and rambles about nothing no reviews or help outside of class the class is boring and he sucks at his job take someone else\n",
            "this teacher is the worst i have ever taken in my whole entire schooling career he is rude belittles students and does not teach what the test is over he puts things on the test that he did not talk about in class and rambles about nothing no reviews or help outside of class the class is boring and he sucks at his job take someone else\n",
            "\n",
            "\n",
            "i had her for math and she was awesome she knows what she is doing and is willing to help you and show you were you made your mistakes if your taking math i recommend taking her \n",
            "i had her for match and she was awesome she knows what she is doing and is willing to help you and show you were you made your mistakes if your taking match i recommend taking her\n",
            "\n",
            "\n",
            "she is a great teacher and enjoys what she does\n",
            "she is a great teacher and enjoys what she does\n",
            "\n",
            "\n",
            "best professor i have had very interested and passionate about what he is teaching there is a lot to learn from him he is not only a quality teacher but a quality human being you will have to work hard in his class but he wants his students to do well \n",
            "best professor i have had very interested and passionate about what he is teaching there is a lot to learn from him he is not only a quality teacher but a quality human being you will have to work hard in his class but he wants his students to do well\n",
            "\n",
            "\n",
            "if you want an easy class take this prof if you want to learn something stay away the prof just liked to talk about whatever was on his mind no direction hardly anything educational he is nice enough but i learned more in a week in another class \n",
            "if you want an easy class take this prof if you want to learn something stay away the prof just liked to talk about whatever was on his mind no direction hardly anything educational he is nice enough but i learned more in a week in another class\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stop Words"
      ],
      "metadata": {
        "id": "XgjB2YuGwxBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# amend list of stop words to keep whatever it is we want by removing words from list that we want to keep\n",
        "\n",
        "# TODO: is the list of stopwords on git complete and accurate or does someone want to read through all 325 stopwords spacy gives and determine which ones to keep?\n",
        "stopwords = STOP_WORDS\n",
        "stopwords.remove(\"but\")\n",
        "stopwords.remove(\"not\")\n",
        "stopwords.remove(\"nor\")\n",
        "stopwords.remove(\"never\")"
      ],
      "metadata": {
        "id": "7q-mMzKxwzeW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_md\", disable=[\"parser\", \"ner\", \"textcat\"])\n",
        "\n",
        "def lemmatize_pipe(doc):\n",
        "    lemma_list = [tok.lemma_ for tok in doc if tok.text not in stopwords]\n",
        "    return lemma_list\n",
        "\n",
        "def preprocess_pipe(texts):\n",
        "    preproc_pipe = []\n",
        "    for doc in nlp.pipe(texts, batch_size=20):\n",
        "        preproc_pipe.append(lemmatize_pipe(doc))\n",
        "    return preproc_pipe"
      ],
      "metadata": {
        "id": "YdCoTZNdxDLv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# no_stopwords = preprocess_pipe(comments_proper)"
      ],
      "metadata": {
        "id": "y45TxHVKAA8e"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews[\"cleanedComment\"] = pd.Series(comments_proper)\n",
        "reviews[\"cleanedCommentChecked\"] = pd.Series(spellchecked_comments)"
      ],
      "metadata": {
        "id": "xOD02O1J8tGW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews[\"sentiment\"] = reviews[\"clarityRating\"].apply(lambda x: 1 if x > 2.5 else 0)       # accuracy: 0.7518830305715551 for 50000 comments\n",
        "#reviews[\"sentiment\"] = reviews[\"clarityRating\"].apply(lambda x: 1 if x > 2.5 else 0)                        # accuracy: 0.8635356668143553 for 50000 comments\n",
        "reviews = reviews.loc[:, [\"firstName\", \"lastName\", \"cleanedComment\", \"cleanedCommentChecked\", \"clarityRating\", \"sentiment\"]]"
      ],
      "metadata": {
        "id": "0w9k2HaH86ne"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes Model"
      ],
      "metadata": {
        "id": "2fgpG8vr9bdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# no_stopwords = [' '.join(lst) for lst in no_stopwords]"
      ],
      "metadata": {
        "id": "mvcJWHoHGhjt"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features=2500, ngram_range=(1,1))\n",
        "X = cv.fit_transform(comments_proper).toarray()"
      ],
      "metadata": {
        "id": "QOPXX-oV9eEU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = pd.get_dummies(reviews.loc[:, [\"sentiment\"]])     # isolate sentiments\n",
        "y = y.loc[:, \"sentiment\"]"
      ],
      "metadata": {
        "id": "b04VIGCr9hFz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
      ],
      "metadata": {
        "id": "-5X1npLi9ldO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "model = MultinomialNB().fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "dZZ6OQFa9nhv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test accuracy of model\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "conf_m = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "acc_score = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy Score: \" + str(acc_score * 100))\n",
        "print(conf_m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bs2o0LnM9pN-",
        "outputId": "70738d63-1a73-4afd-9527-428f95d5833f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score: 85.75624852474495\n",
            "[[ 7390  2451]\n",
            " [ 2980 25308]]\n"
          ]
        }
      ]
    }
  ]
}