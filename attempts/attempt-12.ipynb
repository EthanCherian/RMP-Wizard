{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use negation tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_selection import chi2, SelectPercentile\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from preproc import Preproc\n",
    "import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"../Data/scraped_comments_with_professor.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.drop_duplicates(subset=\"comment_id\", keep=\"first\", inplace=True)\n",
    "\n",
    "reviews.dropna(subset=[\"comment\"], inplace=True)\n",
    "reviews = reviews[reviews[\"comment\"] != \"No Comments\"]\n",
    "\n",
    "reviews['firstName'].fillna('', inplace=True)\n",
    "reviews['lastName'].fillna('', inplace=True)\n",
    "\n",
    "reviews[\"comment\"] = reviews[\"comment\"].apply(lambda x: x if len(x.split()) > 5 else None)\n",
    "reviews.dropna(subset=[\"comment\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 5.0    176812\n",
       " 4.5     58705\n",
       " 4.0     52643\n",
       " 1.0     45128\n",
       " 2.0     26945\n",
       " 3.5     22896\n",
       " 1.5     19230\n",
       " 2.5     17510\n",
       "-1.0         1\n",
       "Name: starRating, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[\"starRating\"] = (reviews[\"clarityRating\"] + reviews[\"helpfulRating\"]) / 2\n",
    "reviews = reviews[reviews[\"starRating\"] != 3.0]         # drop 3 star reviews\n",
    "reviews.reset_index(drop=True, inplace=True)\n",
    "\n",
    "reviews[\"starRating\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words = [\"not\", \"no\", \"nor\", \"neither\"]\n",
    "puncts = [\".\", \",\", \"!\", \"?\"]\n",
    "\n",
    "def negationTag(comment):\n",
    "    new_comment = []\n",
    "    negation_flag = False\n",
    "    for token in nltk.word_tokenize(comment):\n",
    "        if token in negative_words:\n",
    "            negation_flag = True\n",
    "            continue\n",
    "        if negation_flag == True:\n",
    "            if token in puncts:\n",
    "                negation_flag = False\n",
    "            else:\n",
    "                token = \"neg_\" + token\n",
    "        \n",
    "        new_comment.append(token)\n",
    "    \n",
    "    return \" \".join(new_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This class is neg_great . neg_was neg_it neg_informational .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negationTag(\"This class is not very hard. According to a friend, neither are our other classes.\")\n",
    "negationTag(\"This class is not great. nor was it informational.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to convert emoticons and expand contractions using large dictionaries (defined in corpora.py)\n",
    "def convert_emoticons(text):\n",
    "    return corpora.EMOTICONS.get(text, text)\n",
    "\n",
    "def expand_contractions(text):\n",
    "  return corpora.CONTRACTIONS.get(text, text)\n",
    "\n",
    "# helper functions to perform simple regex substitutions\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "\n",
    "def remove_phones(text):\n",
    "    return re.sub(r'\\d{3}-\\d{3}-\\d{4}', ' ', text)\n",
    "\n",
    "def remove_emails(text):\n",
    "    return re.sub(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', ' ', text)\n",
    "\n",
    "def remove_html_entities(text):\n",
    "    text = re.sub('&[0-9a-zA-Z#]+;', ' ', text)\n",
    "    return re.sub('&#63;?', '', text)\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    return re.sub('<.{1,6}?>', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(reviews: pd.DataFrame):\n",
    "    comments_proper = []\n",
    "    unseen = Counter()\n",
    "\n",
    "    for index, review in reviews.iterrows():\n",
    "        comment = review['comment']\n",
    "        fname = review['firstName'].lower().split(' ')\n",
    "        lname = review['lastName'].lower().split(' ')\n",
    "        names = set(fname + lname)\n",
    "\n",
    "        comment = remove_urls(comment)\n",
    "        comment = remove_phones(comment)\n",
    "        comment = remove_emails(comment)\n",
    "        comment = remove_html_entities(comment)\n",
    "        comment = remove_html_tags(comment)\n",
    "\n",
    "        comment_split = comment.split(' ')\n",
    "        new_comment_split = []\n",
    "        for i, word in enumerate(comment_split):\n",
    "            word = convert_emoticons(word)\n",
    "            word = word.lower()\n",
    "            word = expand_contractions(word)\n",
    "            word = re.sub(\"[^a-zA-Z\\s]+\", ' ', word)   # replace characters that are not alphabetic, space, or underscore\n",
    "            word = re.sub(r'(.)\\1\\1+', '\\g<1>', word)  # replace any three character+ sequence with one\n",
    "            word = re.sub('\\s+', ' ', word)\n",
    "            word = word.strip() # trailing whitespace because punctuation replaced by space\n",
    "            new_comment_split.extend(word.split(' '))\n",
    "\n",
    "        # Remove names from the comment\n",
    "        for i, word in enumerate(new_comment_split):\n",
    "            if word in names:\n",
    "                new_comment_split[i] = ''\n",
    "\n",
    "        # putting negation tagging line here improves accuracy *a lot*\n",
    "        # probably bc putting it here essentially leads to negation tagging being ignored lol\n",
    "\n",
    "        comment = ' '.join(new_comment_split)\n",
    "        comment = re.sub('\\s+', ' ', comment)\n",
    "        comment = comment.strip()\n",
    "\n",
    "        comment = negationTag(comment)        # tag words following negation words as negated\n",
    "\n",
    "        comment = ' '.join(word for word in comment.split() if len(word) > 1)\n",
    "\n",
    "        comments_proper.append(comment)\n",
    "\n",
    "    return comments_proper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_proper = preproc(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>professor_id</th>\n",
       "      <th>firstName</th>\n",
       "      <th>lastName</th>\n",
       "      <th>comment</th>\n",
       "      <th>starRating</th>\n",
       "      <th>cleanedComment</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VGVhY2hlci0xMjQzMzQ3</td>\n",
       "      <td>Kurt</td>\n",
       "      <td>Douglass</td>\n",
       "      <td>Good teacher, good lectures. Obviously cares a...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>good teacher good lectures obviously cares abo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VGVhY2hlci0xMjQzMzQ3</td>\n",
       "      <td>Kurt</td>\n",
       "      <td>Douglass</td>\n",
       "      <td>Good teacher, very lenient with grading and at...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>good teacher very lenient with grading and att...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VGVhY2hlci0xMjQzMzQ3</td>\n",
       "      <td>Kurt</td>\n",
       "      <td>Douglass</td>\n",
       "      <td>Very difficult class.  His grading is hard to ...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>very difficult class his grading is hard to un...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VGVhY2hlci0xMDExMDU2</td>\n",
       "      <td>Paula</td>\n",
       "      <td>Zobisch</td>\n",
       "      <td>Excellent mentor. Created valuable foundations...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>excellent mentor created valuable foundations ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VGVhY2hlci0xMDExMDU2</td>\n",
       "      <td>Paula</td>\n",
       "      <td>Zobisch</td>\n",
       "      <td>Awesome...Had her for two classes at baker col...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>awesome had her for two classes at baker colle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           professor_id firstName  lastName  \\\n",
       "0  VGVhY2hlci0xMjQzMzQ3      Kurt  Douglass   \n",
       "1  VGVhY2hlci0xMjQzMzQ3      Kurt  Douglass   \n",
       "2  VGVhY2hlci0xMjQzMzQ3      Kurt  Douglass   \n",
       "3  VGVhY2hlci0xMDExMDU2     Paula   Zobisch   \n",
       "4  VGVhY2hlci0xMDExMDU2     Paula   Zobisch   \n",
       "\n",
       "                                             comment  starRating  \\\n",
       "0  Good teacher, good lectures. Obviously cares a...         5.0   \n",
       "1  Good teacher, very lenient with grading and at...         5.0   \n",
       "2  Very difficult class.  His grading is hard to ...         1.5   \n",
       "3  Excellent mentor. Created valuable foundations...         5.0   \n",
       "4  Awesome...Had her for two classes at baker col...         5.0   \n",
       "\n",
       "                                      cleanedComment  sentiment  \n",
       "0  good teacher good lectures obviously cares abo...          1  \n",
       "1  good teacher very lenient with grading and att...          1  \n",
       "2  very difficult class his grading is hard to un...          0  \n",
       "3  excellent mentor created valuable foundations ...          1  \n",
       "4  awesome had her for two classes at baker colle...          1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = reviews.loc[:, [\"professor_id\", \"firstName\", \"lastName\", \"comment\", \"starRating\"]]\n",
    "reviews[\"cleanedComment\"] = pd.Series(comments_proper)\n",
    "reviews[\"sentiment\"] = reviews[\"starRating\"].apply(lambda x: 1 if x > 2.5 else 0)\n",
    "\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof_counts = reviews[\"professor_id\"].value_counts()\n",
    "prof_train, prof_test, cnt_train, cnt_test = train_test_split(prof_counts.index, prof_counts.values, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_train = reviews[reviews[\"professor_id\"].isin(prof_train)][\"cleanedComment\"]\n",
    "comm_test = reviews[reviews[\"professor_id\"].isin(prof_test)][\"cleanedComment\"]\n",
    "\n",
    "sent_train = reviews[reviews[\"professor_id\"].isin(prof_train)][\"sentiment\"]\n",
    "sent_test = reviews[reviews[\"professor_id\"].isin(prof_test)][\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer(ngram_range=(1,3), min_df=6)), \n",
    "    (\"selector\"  , SelectPercentile(score_func=chi2, percentile=26)),\n",
    "    (\"classifer\" , LogisticRegression(solver=\"liblinear\", C=6))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def evalPerformance(y_pred, y_test, mode=\"weighted\"):\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy: \" + str(acc_score * 100))\n",
    "    \n",
    "    f1 = f1_score(y_test, y_pred, average=mode)\n",
    "    print(\"F1 Score: {0}\\n\".format(f1 * 100))\n",
    "\n",
    "    prec = precision_score(y_test, y_pred, average=mode)\n",
    "    print(\"Precision: {0}\".format(prec * 100))\n",
    "    rec = recall_score(y_test, y_pred, average=mode)\n",
    "    print(\"Recall: {0}\\n\".format(rec * 100))\n",
    "\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_fit = pipeline.fit(comm_train, sent_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_pred = sentiment_fit.predict(comm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.59252362882127\n",
      "F1 Score: 93.53080418969013\n",
      "\n",
      "Precision: 93.52123049182329\n",
      "Recall: 93.59252362882127\n",
      "\n",
      "[[18711  3342]\n",
      " [ 2095 60706]]\n"
     ]
    }
   ],
   "source": [
    "evalPerformance(sent_pred, sent_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_reviews = pd.read_csv(\"../Data/scraped_comments.csv\").sample(n=100000, random_state=1)\n",
    "\n",
    "old_reviews.drop_duplicates(subset=\"comment_id\", keep=\"first\", inplace=True)\n",
    "\n",
    "old_reviews.dropna(subset=[\"comment\"], inplace=True)\n",
    "old_reviews = old_reviews[old_reviews[\"comment\"] != \"No Comments\"]\n",
    "old_reviews[\"comment\"] = old_reviews[\"comment\"].apply(lambda x: x if len(x.split()) > 5 else None)\n",
    "old_reviews.dropna(subset=[\"comment\"], inplace=True)\n",
    "\n",
    "old_reviews.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_reviews[\"starRating\"] = (old_reviews[\"clarityRating\"] + old_reviews[\"helpfulRating\"]) / 2\n",
    "old_reviews = old_reviews[old_reviews[\"starRating\"] != 3.0]         # drop 3 star reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_reviews = old_reviews[old_reviews[\"clarityRating\"] == old_reviews[\"helpfulRating\"]]\n",
    "old_reviews.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_comments = preproc(old_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = sentiment_fit.predict(preproc_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.09139716221398\n",
      "F1 Score: 95.05241925825666\n",
      "\n",
      "Precision: 95.05625205695857\n",
      "Recall: 95.09139716221398\n",
      "\n",
      "[[13294  1710]\n",
      " [  978 38779]]\n"
     ]
    }
   ],
   "source": [
    "real_sentiment = old_reviews[\"starRating\"].apply(lambda x: 1 if x > 2.5 else 0)\n",
    "evalPerformance(preds, real_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ethan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'),\n",
       " ('class', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('pretty', 'JJ'),\n",
       " ('easy', 'JJ'),\n",
       " ('ngl', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"This class is pretty easy ngl.\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ethan\\OneDrive\\Desktop\\NLP\\Attempts\\attempt-12.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ethan/OneDrive/Desktop/NLP/Attempts/attempt-12.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tokenized \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ethan/OneDrive/Desktop/NLP/Attempts/attempt-12.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m comm \u001b[39min\u001b[39;00m reviews[\u001b[39m\"\u001b[39m\u001b[39mcomment\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ethan/OneDrive/Desktop/NLP/Attempts/attempt-12.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     tokenized\u001b[39m.\u001b[39mappend(nltk\u001b[39m.\u001b[39mword_tokenize(comm))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\nltk\\tokenize\\__init__.py:130\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m:type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m--> 130\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m     token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\nltk\\tokenize\\__init__.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m:type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m--> 131\u001b[0m     token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39;49mtokenize(sent)\n\u001b[0;32m    132\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\nltk\\tokenize\\destructive.py:182\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[1;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[0;32m    179\u001b[0m     text \u001b[39m=\u001b[39m regexp\u001b[39m.\u001b[39msub(substitution, text)\n\u001b[0;32m    181\u001b[0m \u001b[39mfor\u001b[39;00m regexp \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCONTRACTIONS2:\n\u001b[1;32m--> 182\u001b[0m     text \u001b[39m=\u001b[39m regexp\u001b[39m.\u001b[39;49msub(\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39m1 \u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39m2 \u001b[39;49m\u001b[39m\"\u001b[39;49m, text)\n\u001b[0;32m    183\u001b[0m \u001b[39mfor\u001b[39;00m regexp \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCONTRACTIONS3:\n\u001b[0;32m    184\u001b[0m     text \u001b[39m=\u001b[39m regexp\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\\\u001b[39m\u001b[39m1 \u001b[39m\u001b[39m\\\u001b[39m\u001b[39m2 \u001b[39m\u001b[39m\"\u001b[39m, text)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\re.py:327\u001b[0m, in \u001b[0;36m_subx\u001b[1;34m(pattern, template)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_subx\u001b[39m(pattern, template):\n\u001b[0;32m    326\u001b[0m     \u001b[39m# internal: Pattern.sub/subn implementation helper\u001b[39;00m\n\u001b[1;32m--> 327\u001b[0m     template \u001b[39m=\u001b[39m _compile_repl(template, pattern)\n\u001b[0;32m    328\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m template[\u001b[39m0\u001b[39m] \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(template[\u001b[39m1\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    329\u001b[0m         \u001b[39m# literal replacement\u001b[39;00m\n\u001b[0;32m    330\u001b[0m         \u001b[39mreturn\u001b[39;00m template[\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenized = []\n",
    "for comm in reviews[\"comment\"]:\n",
    "    tokenized.append(nltk.word_tokenize(comm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_of_speech = []\n",
    "for comm in tokenized:\n",
    "    parts_of_speech.append(nltk.pos_tag(comm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_of_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_of_speech = [item for sublist in parts_of_speech for item in sublist]\n",
    "parts_of_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(tag for (word, tag) in parts_of_speech)\n",
    "words_df = pd.DataFrame({\"POS\":list(fdist.keys()), \"count\":list(fdist.values())})\n",
    "\n",
    "d = words_df.nlargest(columns=\"count\", n=45)\n",
    "plt.figure(figsize=(25,5))\n",
    "# plt.xticks(rotation=45)\n",
    "ax = sns.barplot(data=d, x=\"POS\", y=\"count\")\n",
    "ax.set(ylabel=\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df[\"word\"].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed8ce2715666cca47edfe3dc40f16f58fc193b5a7b06958e2ae06c97afdf9990"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
