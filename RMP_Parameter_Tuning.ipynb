{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQV3OxqpZG_N"
      },
      "source": [
        "# Preprocessing Setup\n",
        "Basic steps, because there are domain specific problems to account for later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7nhxQ8_ZI86"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import spacy\n",
        "import pkg_resources\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from collections import Counter\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import words, wordnet, brown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jDqCGD3s7Ku",
        "outputId": "b9f98287-21d2-4ab8-8071-096c275acea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# !pip install symspellpy\n",
        "\n",
        "from spacy.cli import download\n",
        "download('en_core_web_md')\n",
        "nltk.download('words')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnPL-zIAs8Ml"
      },
      "outputs": [],
      "source": [
        "# from symspellpy import SymSpell, Verbosity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMSveQ7KZ2R_",
        "outputId": "c0d6a63b-4fbf-4108-e883-4b32ea1d8f0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLicijJQZLzr"
      },
      "outputs": [],
      "source": [
        "reviews = pd.read_csv(\"/content/drive/MyDrive/RMP/reviews_filtered.csv\").sample(n=300000, random_state=1)\n",
        "# reviews = pd.read_csv(\"/content/drive/MyDrive/RMP/scraped_comments_with_professor.csv\").sample(n=300000, random_state=1)\n",
        "# reviews[['comment_id', 'firstName', 'lastName', 'prof_class', 'comment', 'clarityRating', 'helpfulRating']].to_csv(\"/content/drive/MyDrive/RMP/scraped_comments_sample.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "9vGcWkCVlghQ",
        "outputId": "0145c97f-6d41-4b95-894f-3371e7bf5fb1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c5ca7b4a-a1a1-4829-ba59-951b4d6a53a9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>firstName</th>\n",
              "      <th>lastName</th>\n",
              "      <th>prof_class</th>\n",
              "      <th>comment</th>\n",
              "      <th>date</th>\n",
              "      <th>clarityRating</th>\n",
              "      <th>helpfulRating</th>\n",
              "      <th>professor_id</th>\n",
              "      <th>comment_id</th>\n",
              "      <th>qualityRating</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>220803</th>\n",
              "      <td>781736</td>\n",
              "      <td>Barbara</td>\n",
              "      <td>Ewell</td>\n",
              "      <td>ENGWRIT</td>\n",
              "      <td>She was very helpful in my writing class.</td>\n",
              "      <td>2012-07-17 14:50:33+00:00</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>VGVhY2hlci05NzE2ODU=</td>\n",
              "      <td>UmF0aW5nLTIwNDk4NjIz</td>\n",
              "      <td>5.0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101502</th>\n",
              "      <td>588101</td>\n",
              "      <td>Jose</td>\n",
              "      <td>Delpilar</td>\n",
              "      <td>PSYCH101</td>\n",
              "      <td>AMAZING PROFESSOR!!!!!!!!!!!!!!!!!!!!! You won...</td>\n",
              "      <td>2011-12-10 00:05:38+00:00</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>VGVhY2hlci0xNTc2Mzg5</td>\n",
              "      <td>UmF0aW5nLTE5NDQ4MDgy</td>\n",
              "      <td>5.0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106342</th>\n",
              "      <td>380054</td>\n",
              "      <td>Samuel</td>\n",
              "      <td>Workman</td>\n",
              "      <td>GOV310L</td>\n",
              "      <td>Such a great professor! He's really helpful an...</td>\n",
              "      <td>2012-01-10 18:17:36+00:00</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>VGVhY2hlci0xNDQ1OTQ4</td>\n",
              "      <td>UmF0aW5nLTE5NzA1NDYx</td>\n",
              "      <td>5.0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66781</th>\n",
              "      <td>173732</td>\n",
              "      <td>Nanete</td>\n",
              "      <td>Maki-Dearsan</td>\n",
              "      <td>ART20</td>\n",
              "      <td>This teacher seemed helpful but then angry whe...</td>\n",
              "      <td>2010-01-26 14:32:30+00:00</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>VGVhY2hlci0xMDI0OTAy</td>\n",
              "      <td>UmF0aW5nLTE2NzU4NzE4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>229545</th>\n",
              "      <td>24293</td>\n",
              "      <td>Catherine</td>\n",
              "      <td>Bacus</td>\n",
              "      <td>GERO11</td>\n",
              "      <td>On Campus Course: Organized, Stays on task, En...</td>\n",
              "      <td>2009-11-24 10:00:38+00:00</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>VGVhY2hlci0xMDg2NjY0</td>\n",
              "      <td>UmF0aW5nLTE2NDIxNzAy</td>\n",
              "      <td>5.0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c5ca7b4a-a1a1-4829-ba59-951b4d6a53a9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c5ca7b4a-a1a1-4829-ba59-951b4d6a53a9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c5ca7b4a-a1a1-4829-ba59-951b4d6a53a9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        Unnamed: 0  firstName      lastName prof_class  \\\n",
              "220803      781736    Barbara         Ewell    ENGWRIT   \n",
              "101502      588101       Jose      Delpilar   PSYCH101   \n",
              "106342      380054     Samuel       Workman    GOV310L   \n",
              "66781       173732     Nanete  Maki-Dearsan      ART20   \n",
              "229545       24293  Catherine         Bacus     GERO11   \n",
              "\n",
              "                                                  comment  \\\n",
              "220803          She was very helpful in my writing class.   \n",
              "101502  AMAZING PROFESSOR!!!!!!!!!!!!!!!!!!!!! You won...   \n",
              "106342  Such a great professor! He's really helpful an...   \n",
              "66781   This teacher seemed helpful but then angry whe...   \n",
              "229545  On Campus Course: Organized, Stays on task, En...   \n",
              "\n",
              "                             date  clarityRating  helpfulRating  \\\n",
              "220803  2012-07-17 14:50:33+00:00              5              5   \n",
              "101502  2011-12-10 00:05:38+00:00              5              5   \n",
              "106342  2012-01-10 18:17:36+00:00              5              5   \n",
              "66781   2010-01-26 14:32:30+00:00              2              2   \n",
              "229545  2009-11-24 10:00:38+00:00              5              5   \n",
              "\n",
              "                professor_id            comment_id  qualityRating  sentiment  \n",
              "220803  VGVhY2hlci05NzE2ODU=  UmF0aW5nLTIwNDk4NjIz            5.0       True  \n",
              "101502  VGVhY2hlci0xNTc2Mzg5  UmF0aW5nLTE5NDQ4MDgy            5.0       True  \n",
              "106342  VGVhY2hlci0xNDQ1OTQ4  UmF0aW5nLTE5NzA1NDYx            5.0       True  \n",
              "66781   VGVhY2hlci0xMDI0OTAy  UmF0aW5nLTE2NzU4NzE4            2.0      False  \n",
              "229545  VGVhY2hlci0xMDg2NjY0  UmF0aW5nLTE2NDIxNzAy            5.0       True  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reviews.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGS9yvAMag7H"
      },
      "source": [
        "## Duplicates and Nulls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLUgn5sbZ_1g",
        "outputId": "51c36d53-e037-4e06-9663-b4f8d6b51da4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape before dropping: (300000, 12)\n",
            "Shape after dropping: (300000, 12)\n"
          ]
        }
      ],
      "source": [
        "print(\"Shape before dropping:\", reviews.shape)\n",
        "reviews.drop_duplicates(subset=\"comment_id\", keep=\"first\", inplace=True)\n",
        "\n",
        "# drop rows containing only \"No Comments\" (default value assigned by RMP to a review that didn't enter a comment)\n",
        "reviews = reviews[reviews[\"comment\"] != \"No Comments\"]\n",
        "\n",
        "# drop rows containing NaN comment\n",
        "reviews.dropna(subset=[\"comment\"], inplace=True)\n",
        "\n",
        "# fill null names with empty string\n",
        "reviews['firstName'].fillna('', inplace=True)\n",
        "reviews['lastName'].fillna('', inplace=True)\n",
        "\n",
        "# Dropping reviews with qualityRating == 3.0\n",
        "reviews['qualityRating'] = (reviews['helpfulRating']+reviews['clarityRating'])/2.0\n",
        "reviews = reviews[reviews[\"qualityRating\"] != 3.0]\n",
        "reviews[\"sentiment\"] = reviews[\"qualityRating\"] > 3.0\n",
        "\n",
        "print(\"Shape after dropping:\", reviews.shape)\n",
        "reviews.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDFaJC7Fe4nk"
      },
      "source": [
        "## Removing Urls, Phone Numbers, and Emails"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzZSR-eXe9Cr",
        "outputId": "c404c133-78f2-45c2-ef9d-d041595efe67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hey! Check out this link:  \n",
            "Hey! Check out this phone number:  \n",
            "Hey! Check out this email address:  \n"
          ]
        }
      ],
      "source": [
        "def remove_urls(text):\n",
        "    return re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
        "\n",
        "def remove_phones(text):\n",
        "    return re.sub(r'\\d{3}-\\d{3}-\\d{4}', ' ', text)\n",
        "\n",
        "def remove_emails(text):\n",
        "    return re.sub(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', ' ', text)\n",
        "\n",
        "print(remove_urls('Hey! Check out this link: www.somelink.com'))\n",
        "print(remove_phones(\"Hey! Check out this phone number: 742-457-0417\"))\n",
        "print(remove_emails(\"Hey! Check out this email address: nooneuses@yahoo.com\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDTjxLjqnu-B"
      },
      "source": [
        "## Html Artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEmTjY8mnzaQ",
        "outputId": "92c2e7be-77f0-443b-b5e5-0be617f6f48a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This professor is such an easy  A , why are y'all struggling    </div>\n",
            "This professor is such an easy &quot;A&quot;, why are y'all struggling &#63;&#63;&#63  \n"
          ]
        }
      ],
      "source": [
        "# TODO: Convert html entites of quotes -> \"'\" to normalize\n",
        "def remove_html_entities(text):\n",
        "  text = re.sub('&[0-9a-zA-Z#]+;', ' ', text)\n",
        "  return re.sub('&#63;?', '', text)\n",
        "\n",
        "def remove_html_tags(text):\n",
        "  return re.sub('<.{1,6}?>', ' ', text)\n",
        "\n",
        "text = \"This professor is such an easy &quot;A&quot;, why are y'all struggling &#63;&#63;&#63 </div>\"\n",
        "print(remove_html_entities(text))\n",
        "print(remove_html_tags(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxBEJJO9a-Jo"
      },
      "source": [
        "## Emoticon Conversion to Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rf2Q8b2TbAsr"
      },
      "outputs": [],
      "source": [
        "#@title Emoticon Mapping\n",
        "EMOTICONS = {\n",
        "    u\"<3\": \"emopos\",\n",
        "    u\":‑)\":\"emopos\",\n",
        "    u\":-))\":\"emopos\",\n",
        "    u\":-)))\":\"emopos\",\n",
        "    u\":)\":\"emopos\",\n",
        "    u\":))\":\"emopos\",\n",
        "    u\":)))\":\"emopos\",\n",
        "    u\":-]\":\"emopos\",\n",
        "    u\":]\":\"emopos\",\n",
        "    u\":-3\":\"emopos\",\n",
        "    u\":3\":\"emopos\",\n",
        "    u\":->\":\"emopos\",\n",
        "    u\":>\":\"emopos\",\n",
        "    u\"8-)\":\"emopos\",\n",
        "    u\":-}\":\"emopos\",\n",
        "    u\":}\":\"emopos\",\n",
        "    u\":-)\":\"emopos\",\n",
        "    u\":c)\":\"emopos\",\n",
        "    u\":^)\":\"emopos\",\n",
        "    u\"=]\":\"emopos\",\n",
        "    u\"=)\":\"emopos\",\n",
        "    u\":‑D\":\"emopos\",\n",
        "    u\":D\":\"emopos\",\n",
        "    u\"8‑D\":\"emopos\",\n",
        "    u\"8D\":\"emopos\",\n",
        "    u\"X‑D\":\"emopos\",\n",
        "    u\"XD\":\"emopos\",\n",
        "    u\"=D\":\"emopos\",\n",
        "    u\"=3\":\"emopos\",\n",
        "    u\"B^D\":\"emopos\",\n",
        "    u\":-))\":\"emopos\",\n",
        "    u\":-(\":\"emoneg\",\n",
        "    u\":‑(\":\"emoneg\",\n",
        "    u\":(\":\"emoneg\",\n",
        "    u\":‑c\":\"emoneg\",\n",
        "    u\":c\":\"emoneg\",\n",
        "    u\":‑<\":\"emoneg\",\n",
        "    u\":<\":\"emoneg\",\n",
        "    u\":‑[\":\"emoneg\",\n",
        "    u\":[\":\"emoneg\",\n",
        "    u\":-||\":\"emoneg\",\n",
        "    u\">:[\":\"emoneg\",\n",
        "    u\":{\":\"emoneg\",\n",
        "    u\">:(\":\"emoneg\",\n",
        "    u\":'‑(\":\"emoneg\",\n",
        "    u\":'(\":\"emoneg\",\n",
        "    u\":'‑)\":\"emopos\",\n",
        "    u\":')\":\"emopos\",\n",
        "    u\"D‑':\":\"emoneg\",\n",
        "    u\"D:<\":\"emoneg\",\n",
        "    u\"D:\":\"emoneg\",\n",
        "    u\"D8\":\"emoneg\",\n",
        "    u\"D;\":\"emoneg\",\n",
        "    u\"D=\":\"emoneg\",\n",
        "    u\"DX\":\"emoneg\",\n",
        "    u\";‑)\":\"emopos\",\n",
        "    u\";)\":\"emopos\",\n",
        "    u\"*-)\":\"emopos\",\n",
        "    u\"*)\":\"emopos\",\n",
        "    u\";‑]\":\"emopos\",\n",
        "    u\";]\":\"emopos\",\n",
        "    u\";^)\":\"emopos\",\n",
        "    u\":‑,\":\"emopos\",\n",
        "    u\";D\":\"emopos\",\n",
        "    u\":‑P\":\"emopos\",\n",
        "    u\":P\":\"emopos\",\n",
        "    u\"X‑P\":\"emopos\",\n",
        "    u\"XP\":\"emopos\",\n",
        "    u\":‑Þ\":\"emopos\",\n",
        "    u\":Þ\":\"emopos\",\n",
        "    u\"=p\":\"emopos\",\n",
        "    u\":‑/\":\"emoneg\",\n",
        "    u\":/\":\"emoneg\",\n",
        "    u\":-[.]\":\"emoneg\",\n",
        "    u\">:[(\\)]\":\"emoneg\",\n",
        "    u\">:/\":\"emoneg\",\n",
        "    u\":[(\\)]\":\"emoneg\",\n",
        "    u\"=/\":\"emoneg\",\n",
        "    u\"=[(\\)]\":\"emoneg\",\n",
        "    u\":L\":\"emoneg\",\n",
        "    u\"=L\":\"emoneg\",\n",
        "    u\":‑|\":\"emoneg\",\n",
        "    u\":|\":\"emoneg\",\n",
        "    u\"O:‑)\":\"emopos\",\n",
        "    u\"O:)\":\"emopos\",\n",
        "    u\"0:‑3\":\"emopos\",\n",
        "    u\"0:3\":\"emopos\",\n",
        "    u\"0:‑)\":\"emopos\",\n",
        "    u\"0:)\":\"emopos\",\n",
        "    u\":‑b\":\"emopos\",\n",
        "    u\"(>_<)\":\"emoneg\",\n",
        "    u\"(>_<)>\":\"emoneg\",\n",
        "    u\"^_^\":\"emopos\",\n",
        "    u\"(^_^)/\":\"emopos\",\n",
        "    u\"(^O^)／\":\"emopos\",\n",
        "    u\"(^o^)／\":\"emopos\",\n",
        "    u\"('_')\":\"emoneg\",\n",
        "    u\"(/_;)\":\"emoneg\",\n",
        "    u\"(T_T) (;_;)\":\"emoneg\",\n",
        "    u\"(;_;\":\"emoneg\",\n",
        "    u\"(;_:)\":\"emoneg\",\n",
        "    u\"(;O;)\":\"emoneg\",\n",
        "    u\"(:_;)\":\"emoneg\",\n",
        "    u\"(ToT)\":\"emoneg\",\n",
        "    u\";_;\":\"emoneg\",\n",
        "    u\";-;\":\"emoneg\",\n",
        "    u\";n;\":\"emoneg\",\n",
        "    u\"Q.Q\":\"emoneg\",\n",
        "    u\"T.T\":\"emoneg\",\n",
        "    u\"Q_Q\":\"emoneg\",\n",
        "    u\"(-.-)\":\"emopos\",\n",
        "    u\"(-_-)\":\"emopos\",\n",
        "    u\"(；一_一)\":\"emopos\",\n",
        "    u\"(=_=)\":\"emoneg\",\n",
        "    u\"^m^\":\"emopos\",\n",
        "    u\">^_^<\":\"emopos\",\n",
        "    u\"<^!^>\":\"emopos\",\n",
        "    u\"^/^\":\"emopos\",\n",
        "    u\"（*^_^*）\" :\"emopos\",\n",
        "    u\"(^<^) (^.^)\":\"emopos\",\n",
        "    u\"(^^)\":\"emopos\",\n",
        "    u\"(^.^)\":\"emopos\",\n",
        "    u\"(^_^.)\":\"emopos\",\n",
        "    u\"(^_^)\":\"emopos\",\n",
        "    u\"(^^)\":\"emopos\",\n",
        "    u\"(^J^)\":\"emopos\",\n",
        "    u\"(*^.^*)\":\"emopos\",\n",
        "    u\"(^—^）\":\"emopos\",\n",
        "    u\"(#^.^#)\":\"emopos\",\n",
        "    u\"(*^0^*)\":\"emopos\",\n",
        "    u\"(*^^)v\":\"emopos\",\n",
        "    u\"(^_^)v\":\"emopos\",\n",
        "    u'(-\"-)':\"emoneg\",\n",
        "    u\"(ーー;)\":\"emoneg\",\n",
        "    u\"(＾ｖ＾)\":\"emopos\",\n",
        "    u\"(＾ｕ＾)\":\"emopos\",\n",
        "    u\"(^)o(^)\":\"emopos\",\n",
        "    u\"(^O^)\":\"emopos\",\n",
        "    u\"(^o^)\":\"emopos\",\n",
        "    u\")^o^(\":\"emopos\",\n",
        "    u\":O o_O\":\"emoneg\",\n",
        "    u\"o_0\":\"emoneg\",\n",
        "    u\"o.O\":\"emoneg\",\n",
        "    u\"(o.o)\":\"emoneg\",\n",
        "    u\"(*￣m￣)\": \"emoneg\",\n",
        "}\n",
        "\n",
        "for emote, val in EMOTICONS.items():\n",
        "  EMOTICONS[emote] = val.lower().replace(',', ' ').replace(' ', '_')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-jJdrc-bpp9",
        "outputId": "77737b73-c6fe-4f24-a9f2-a49ad0af35db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello emopos emopos\n"
          ]
        }
      ],
      "source": [
        "def convert_emoticons(text):\n",
        "  return EMOTICONS.get(text, text)\n",
        "  \n",
        "text = \"Hello :-) :-)\"\n",
        "text_split = text.split()\n",
        "for i, txt in enumerate(text_split):\n",
        "  text_split[i] = convert_emoticons(txt)\n",
        "print(' '.join(text_split))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9wPsJ3ghEBw"
      },
      "source": [
        "## Contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgcFh25YhG5U"
      },
      "outputs": [],
      "source": [
        "#@title Contraction Mapping\n",
        "contraction_mapping = {\n",
        "    \"dont\": \"do not\",\n",
        "    \"doesnt\": \"does not\",\n",
        "    \"arent\": \"are not\",\n",
        "    \"cant\": \"can not\",\n",
        "    \"couldve\": \"could have\",\n",
        "    \"couldnt\": \"could not\",\n",
        "    \"didnt\": \"did not\",\n",
        "    \"aint\": \"is not\",\n",
        "    \"arent\": \"are not\",\n",
        "    \"hes\": \"he is\",\n",
        "    \"shes\": \"she is\",\n",
        "    \"havent\": \"have not\",\n",
        "    \"hasnt\": \"has not\",\n",
        "    'youll': \"you will\",\n",
        "    \"ive\": \"i have\",\n",
        "    \"youve\": \"you have\",\n",
        "    \"shouldve\": \"should have\",\n",
        "    \"im\": \"i am\",\n",
        "    \"isnt\": \"is not\",\n",
        "    \"ain't\": \"is not\", \n",
        "    \"aren't\": \"are not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"'cause\": \"because\", \n",
        "    \"could've\": \"could have\", \n",
        "    \"couldn't\": \"could not\", \n",
        "    \"didn't\": \"did not\",  \n",
        "    \"doesn't\": \"does not\", \n",
        "    \"don't\": \"do not\", \n",
        "    \"hadn't\": \"had not\", \n",
        "    \"hasn't\": \"has not\", \n",
        "    \"shouldnt\": \"should not\",\n",
        "    \"haven't\": \"have not\", \n",
        "    \"he'd\": \"he would\",\n",
        "    \"he'll\": \"he will\", \n",
        "    \"he's\": \"he is\", \n",
        "    \"how'd\": \"how did\", \n",
        "    \"how'd'y\": \"how do you\", \n",
        "    \"how'll\": \"how will\", \n",
        "    \"how's\": \"how is\",\n",
        "    \"i'd\": \"i would\", \n",
        "    \"i'd've\": \"i would have\", \n",
        "    \"i'll\": \"i will\",  \n",
        "    \"i'll've\": \"i will have\",\n",
        "    \"i'm\": \"i am\", \n",
        "    \"i've\": \"i have\", \n",
        "    \"isn't\": \"is not\", \n",
        "    \"it'd\": \"it would\", \n",
        "    \"it'd've\": \"it would have\", \n",
        "    \"it'll\": \"it will\", \n",
        "    \"it'll've\": \"it will have\",\n",
        "    \"it's\": \"it is\", \n",
        "    \"let's\": \"let us\", \n",
        "    \"ma'am\": \"madam\", \n",
        "    \"mayn't\": \"may not\", \n",
        "    \"might've\": \"might have\",\n",
        "    \"mightn't\": \"might not\",\n",
        "    \"mightn't've\": \"might not have\", \n",
        "    \"must've\": \"must have\", \n",
        "    \"mustn't\": \"must not\", \n",
        "    \"mustn't've\": \"must not have\", \n",
        "    \"needn't\": \"need not\", \n",
        "    \"needn't've\": \"need not have\",\n",
        "    \"o'clock\": \"of the clock\", \n",
        "    \"oughtn't\": \"ought not\", \n",
        "    \"oughtn't've\": \"ought not have\", \n",
        "    \"shan't\": \"shall not\", \n",
        "    \"sha'n't\": \"shall not\", \n",
        "    \"shan't've\": \"shall not have\", \n",
        "    \"she'd\": \"she would\", \n",
        "    \"she'd've\": \"she would have\", \n",
        "    \"she'll\": \"she will\", \n",
        "    \"she'll've\": \"she will have\", \n",
        "    \"she's\": \"she is\", \n",
        "    \"should've\": \"should have\", \n",
        "    \"shouldn't\": \"should not\", \n",
        "    \"shouldn't've\": \"should not have\", \n",
        "    \"this's\": \"this is\",\n",
        "    \"this'll\": \"this will\",\n",
        "    \"thisll\": \"this will\",\n",
        "    \"that'd\": \"that would\", \n",
        "    \"that'd've\": \"that would have\", \n",
        "    \"that's\": \"that is\", \n",
        "    \"thats\": \"that is\",\n",
        "    \"there'd\": \"there would\", \n",
        "    \"there'd've\": \"there would have\", \n",
        "    \"there's\": \"there is\", \n",
        "    \"theres\": \"there is\",\n",
        "    \"here's\": \"here is\",\n",
        "    \"heres\": \"here is\",\n",
        "    \"they'd\": \"they would\", \n",
        "    \"they'd've\": \"they would have\", \n",
        "    \"they'll\": \"they will\", \n",
        "    \"they'll've\": \"they will have\", \n",
        "    \"they're\": \"they are\", \n",
        "    \"they've\": \"they have\", \n",
        "    \"to've\": \"to have\", \n",
        "    \"wasn't\": \"was not\", \n",
        "    \"wasnt\": \"was not\",\n",
        "    \"we'd\": \"we would\", \n",
        "    \"we'd've\": \"we would have\", \n",
        "    \"we'll\": \"we will\", \n",
        "    \"we'll've\": \"we will have\", \n",
        "    \"we're\": \"we are\", \n",
        "    \"we've\": \"we have\", \n",
        "    \"weve\": \"we have\",\n",
        "    \"werent\": \"were not\",\n",
        "    \"weren't\": \"were not\", \n",
        "    \"what'll\": \"what will\",\n",
        "    \"whatll\": \"what will\",\n",
        "    \"what'll've\": \"what will have\", \n",
        "    \"what're\": \"what are\",  \n",
        "    \"what's\": \"what is\", \n",
        "    \"what's\": \"what is\",\n",
        "    \"whatve\": \"what have\",\n",
        "    \"what've\": \"what have\", \n",
        "    \"when's\": \"when is\", \n",
        "    \"when've\": \"when have\", \n",
        "    \"where'd\": \"where did\", \n",
        "    \"where's\": \"where is\", \n",
        "    \"wheres\": \"where is\", \n",
        "    \"where've\": \"where have\",\n",
        "    \"who'll\": \"who will\", \n",
        "    \"who'll've\": \"who will have\", \n",
        "    \"who's\": \"who is\", \n",
        "    \"who've\": \"who have\", \n",
        "    \"whys\": \"why is\",\n",
        "    \"why's\": \"why is\", \n",
        "    \"why've\": \"why have\", \n",
        "    \"will've\": \"will have\", \n",
        "    \"willve\": \"will have\",\n",
        "    \"won't\": \"will not\", \n",
        "    \"wont\": \"will not\",\n",
        "    \"won't've\": \"will not have\", \n",
        "    \"would've\": \"would have\", \n",
        "    \"wouldve\": \"would have\",\n",
        "    \"wouldn't\": \"would not\", \n",
        "    \"wouldnt\": \"would not\",\n",
        "    \"wouldn't've\": \"would not have\", \n",
        "    \"y'all\": \"you all\", \n",
        "    \"y'all'd\": \"you all would\",\n",
        "    \"y'all'd've\": \"you all would have\",\n",
        "    \"y'all're\": \"you all are\",\n",
        "    \"y'all've\": \"you all have\",\n",
        "    \"you'd\": \"you would\", \n",
        "    \"you'd've\": \"you would have\", \n",
        "    \"you'll\": \"you will\", \n",
        "    \"you'll've\": \"you will have\", \n",
        "    \"you're\": \"you are\", \n",
        "    \"you've\": \"you have\"\n",
        "}\n",
        "\n",
        "len(contraction_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rydaj1_hLuSL",
        "outputId": "e20d9378-e90b-4692-d889-b9a571f60ac1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "120"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(contraction_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl1TXvlvmjdx",
        "outputId": "79b2f9a5-3060-4b06-ae3d-f32662a85d80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "you are a pig and i should have slayed you, grrr\n"
          ]
        }
      ],
      "source": [
        "def expand_contraction(text): # Before expanding contraction, might want to clean of symbols that are not '\n",
        "  return contraction_mapping.get(text, text)\n",
        "\n",
        "text = \"You're a pig and I should've slayed you, grrr\"\n",
        "text_split = text.split()\n",
        "for i, txt in enumerate(text_split):\n",
        "  text_split[i] = expand_contraction(txt.lower())\n",
        "print(' '.join(text_split))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gt7C3v2wNuNN"
      },
      "source": [
        "## Slang/Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c8Slb5mNx8U"
      },
      "outputs": [],
      "source": [
        "#@title Vocab Mapping\n",
        "vocab_mapping = {\n",
        "    'ta': 'teaching assistant',\n",
        "    'biz': 'business',\n",
        "    'hw': 'homework',\n",
        "    'hws': 'homeworks',\n",
        "    'faq': 'frequently answered question',\n",
        "    'faqs': 'frequently answered questions',\n",
        "    'mcq': 'multiple choice question',\n",
        "    'mcqs': 'multiple choice questions',\n",
        "    'frq': 'free response question',\n",
        "    'frqs': 'free response questions',\n",
        "    'ppt': 'powerpoint',\n",
        "    'ppts': 'powerpoints',\n",
        "    'ques': 'question',\n",
        "    'bs': 'bullshit',\n",
        "    'bsing': 'bullshitting',\n",
        "    'bsed': 'bullshitted',\n",
        "    'lol': 'laugh out loud',\n",
        "    'btw': 'by the way',\n",
        "    'imo': 'in my opinion',\n",
        "    'imho': 'in my honest opinion',\n",
        "    'tbh': 'to be honest',\n",
        "    'asap': 'as soon as possible',\n",
        "    'idc': 'i do not care',\n",
        "    'omg': 'oh my god',\n",
        "    'ppl': 'people',\n",
        "    'rip': 'rest in peace',\n",
        "    'srsly': 'seriously',\n",
        "    'thx': 'thanks',\n",
        "    'txt': 'text',\n",
        "    'ur': 'your',\n",
        "    'tho': 'though',\n",
        "    'wtf': 'what the fuck',\n",
        "    'wth': 'what the heck',\n",
        "    'bc': 'because',\n",
        "    'b4': 'before',\n",
        "    'h8': 'hate',\n",
        "    'jk': 'just kidding',\n",
        "    'cuz': 'because'\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWtsBJUGtPK8"
      },
      "source": [
        "## Spellchecker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKD7XvKttROM"
      },
      "outputs": [],
      "source": [
        "# sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "# dictionary_path = pkg_resources.resource_filename(\n",
        "#     \"symspellpy\", \"frequency_dictionary_en_82_765.txt\"\n",
        "# )\n",
        "# # term_index is the column of the term and count_index is the\n",
        "# # column of the term frequency\n",
        "# sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "\n",
        "# # lookup suggestions for single-word input strings\n",
        "# input_term = \"memebers\"  # misspelling of \"members\"\n",
        "\n",
        "# # Verbosity.TOP gets the best suggestion\n",
        "# suggestion = sym_spell.lookup(input_term, Verbosity.TOP, max_edit_distance=2)\n",
        "# print(suggestion[0], len(suggestion))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q3TYcnvmjP7"
      },
      "source": [
        "## Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYOoJLdAmkw_"
      },
      "outputs": [],
      "source": [
        "# amend list of stop words to keep whatever it is we want by removing words from list that we want to keep\n",
        "\n",
        "# TODO: is the list of stopwords on git complete and accurate or does someone want to read through all 325 stopwords spacy gives and determine which ones to keep?\n",
        "# stopwords = STOP_WORDS\n",
        "# stopwords.remove(\"but\")\n",
        "# stopwords.remove(\"not\")\n",
        "# stopwords.remove(\"nor\")\n",
        "# stopwords.remove(\"never\")\n",
        "gen_stops = set([\"mr\", \"ms\", \"dr\", \"doctor\", \"s\", \"t\", \"i\", \"me\", \"myself\", \"is\", \"she\", \"he\", \"we\", \"him\", \"her\", \"it\"])\n",
        "domain_stops = set([\"book\", \"books\", \"college\", \"colleges\", \"lecture\", \"lectures\", \"university\", \"universities\", \"lab\", \"labs\", \"hw\", \"hws\", \"quiz\", \"quizzes\", \"prof\", \"professor\", \"teacher\", \"class\", \"classes\", \"course\", \"courses\"])\n",
        "stopwords = gen_stops.union(domain_stops)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT1TVLy465BY"
      },
      "source": [
        "## Spacy Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyioHaPm67ik"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm', exclude=['lemmatizer', 'parser', 'textcat', 'custom'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOf9dOyURHNm"
      },
      "source": [
        "# Undersampling\n",
        "Currently the method used to undersample is messy.\n",
        "Not only this, but with multinomial nb the stats are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2-48HYpRGsX",
        "outputId": "b78a7765-d56f-475f-ef73-3a941b46e441"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "221784 78216\n",
            "182504\n",
            "True     182504\n",
            "False     78216\n",
            "Name: sentiment, dtype: int64\n",
            "0.7\n"
          ]
        }
      ],
      "source": [
        "reviews_pos = reviews[reviews['sentiment'] == 1]\n",
        "reviews_neg = reviews[reviews['sentiment'] == 0]\n",
        "print(len(reviews_pos), len(reviews_neg))\n",
        "reviews_pos = reviews[reviews['sentiment'] == 1].sample(n = int(len(reviews_neg)*(.7/.3)), random_state=1) # Messy way of undersampling\n",
        "\n",
        "print(len(reviews_pos))\n",
        "\n",
        "reviews_pos.reset_index(inplace=True, drop=True)\n",
        "reviews_neg.reset_index(inplace=True, drop=True)\n",
        "reviews = pd.concat([reviews_pos, reviews_neg], ignore_index=True)\n",
        "\n",
        "print(reviews.sentiment.value_counts())\n",
        "print(reviews.sentiment.value_counts()[True]/len(reviews))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8c1-4PfqaCa"
      },
      "source": [
        "# Preprocessing Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hI6lvtrgqc0i",
        "outputId": "da793a3b-077b-4ff4-9ec6-72742f3e2956"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25000\n",
            "50000\n",
            "75000\n",
            "100000\n",
            "125000\n",
            "150000\n",
            "175000\n",
            "200000\n",
            "225000\n",
            "250000\n"
          ]
        }
      ],
      "source": [
        "spellchecked_comments = []\n",
        "lemm = WordNetLemmatizer()\n",
        "grades = set(['a', 'b', 'c', 'd', 'e', 'f'])\n",
        "unseen = Counter()\n",
        "\n",
        "def preprocess_pipeline(df):\n",
        "  cnt = 0 # to keep track of progress\n",
        "  comments_proper = []\n",
        "  for index, review in df.iterrows():\n",
        "    comment = review['comment']\n",
        "    fname = review['firstName'].lower().split(' ')\n",
        "    lname = review['lastName'].lower().split(' ')\n",
        "    names = set(fname + lname)\n",
        "\n",
        "    cnt += 1\n",
        "    if cnt % 25000 == 0:\n",
        "      print(cnt)\n",
        "\n",
        "    comment = remove_urls(comment)\n",
        "    comment = remove_phones(comment)\n",
        "    comment = remove_emails(comment)\n",
        "    comment = remove_html_entities(comment)\n",
        "    comment = remove_html_tags(comment)\n",
        "\n",
        "    comment_split = comment.split(' ')\n",
        "    new_comment_split = []\n",
        "    for i, word in enumerate(comment_split):\n",
        "      word = convert_emoticons(word)\n",
        "      word = word.lower()\n",
        "      word = expand_contraction(word)\n",
        "      word = re.sub(\"[^a-z\\s]+\", ' ', word)   # replace characters that are not alphabetic, space, or underscore\n",
        "      # word = word.replace(\"'\", ' ') # replace apostrophe with space\n",
        "      word = re.sub(r'(.)\\1\\1+', '\\g<1>', word)  # replace any three character+ sequence with one\n",
        "      word = re.sub('\\s+', ' ', word)\n",
        "      word = word.strip() # trailing whitespace because punctuation replaced by space\n",
        "      # if word not in names:\n",
        "      new_comment_split.extend(word.split(' '))\n",
        "\n",
        "    # comment = comment.lower()\n",
        "    # comment = re.sub(\"[^a-zA-Z\\s]+\", ' ', comment)   # replace characters that are not alphabetic, space, or underscore\n",
        "    # comment = comment.replace(\"'\", '') # remove apostrophes\n",
        "    # comment = re.sub(r'(.)\\1\\1+', '\\g<1>', comment)  # replace any three characters sequence with one\n",
        "    # comment = re.sub('\\s+', ' ', comment)\n",
        "    # comment = comment.strip() # trailing whitespace because punctuation replaced by space\n",
        "\n",
        "    \n",
        "    # comment_split = comment.split(' ')\n",
        "    # new_comment_split = []\n",
        "    # for i, word in enumerate(comment_split):\n",
        "    #   if word not in names:\n",
        "    #     new_comment_split.append(word)\n",
        "\n",
        "    # Remove names from the comment\n",
        "    for i, word in enumerate(new_comment_split):\n",
        "      if word in names or word in stopwords:\n",
        "        new_comment_split[i] = ''\n",
        "\n",
        "    comment = ' '.join(new_comment_split)\n",
        "    comment = re.sub('\\s+', ' ', comment)\n",
        "    comment = comment.strip()\n",
        "\n",
        "    # comment = [lemm.lemmatize(word) for word in comment.split()] # Lemmatize\n",
        "    # comment = [word for word in comment.split() if word not in stopwords] # remove stopwords\n",
        "    # comment = \" \".join(comment)\n",
        "\n",
        "    # comment = ' '.join(word for word in comment.split() if len(word) > 1)\n",
        "\n",
        "    comments_proper.append(comment)\n",
        "    # spellchecked_comments.append(' '.join(sym_spell.lookup(word, Verbosity.TOP, max_edit_distance=2, include_unknown=True)[0].term for word in comment.split()))\n",
        "  return comments_proper\n",
        "\n",
        "comments_proper = preprocess_pipeline(reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YUgaFJgA4VL"
      },
      "outputs": [],
      "source": [
        "unseen_words = Counter()\n",
        "people = Counter()\n",
        "def preprocess_pipe(texts):\n",
        "    preproc_pipe = []\n",
        "    for doc in nlp.pipe(texts, batch_size=200):\n",
        "      # for word in doc:\n",
        "      #   if word.pos_ == 'PROPN':\n",
        "      #     unseen_words[word.text] += 1\n",
        "      #     print(word.text, word.pos_)\n",
        "      # print(doc.ents)\n",
        "      for word in doc.ents:\n",
        "        if word.label_ == 'PERSON':\n",
        "          people[word.text] += 1\n",
        "          # print(word.text,word.label_)\n",
        "\n",
        "# preprocess_pipe(comments_proper)\n",
        "# print(unseen_words, len(unseen_words))\n",
        "# print(people, len(people))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoPSuw4tS6hA",
        "outputId": "bcb882d4-4a6b-4388-a677-38feaf24911f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter()\n"
          ]
        }
      ],
      "source": [
        "print(people)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEQYPIoJsS7_"
      },
      "outputs": [],
      "source": [
        "# i = 0\n",
        "# for comment, spellcheck_comment in zip(comments_proper, spellchecked_comments):\n",
        "#   print(comment)\n",
        "#   print(spellcheck_comment)\n",
        "#   print('\\n')\n",
        "#   i += 1\n",
        "#   if i == 10:\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOD02O1J8tGW",
        "outputId": "803d90da-1f98-4602-88da-00e37ece0e3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0     really nice boring but if you attend his you w...\n",
              "1     pretty easy all grades you on tests with the t...\n",
              "2     this amazing very knowledgeable very helpful f...\n",
              "3           very inspirational caring and understanding\n",
              "4     awesome not a complete blow off but pretty dam...\n",
              "5     took speech from back in at golden west in cal...\n",
              "6     very clear when teaches sit in the front row b...\n",
              "7                  great that you can easily relate too\n",
              "8     great took this online very clear on what requ...\n",
              "9     really cares for students expects you to work ...\n",
              "10    was by far the nicest and most caring that hav...\n",
              "11            made interesting and fun highly recommend\n",
              "12    should preface this review by saying that have...\n",
              "13                                      liked his kinda\n",
              "14    took his at the of kentucky not murray state a...\n",
              "15    took ochem and during the summer and got a in ...\n",
              "16             amazing loved study and you will do well\n",
              "17    an awesome does not just go over power point l...\n",
              "18    dont recommend unhelpful bs assignments thinks...\n",
              "19    one of the nicest profs def willing to help yo...\n",
              "20    loved was nice not too hard and fair hates whe...\n",
              "21    the sweetest woman loved interesting and had r...\n",
              "22    enjoyed taking orgo with a really good and lik...\n",
              "23    online not hard but also not easy if you do al...\n",
              "24    very straightforward requires in depth study o...\n",
              "Name: cleanedComment, dtype: object"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reviews[\"cleanedComment\"] = pd.Series(comments_proper)\n",
        "# reviews[\"cleanedCommentChecked\"] = pd.Series(spellchecked_comments)\n",
        "reviews['cleanedComment'].head(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnRBko2cixUB",
        "outputId": "63032a1b-97a7-459b-eead-ed8d4efa38ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True     182504\n",
              "False     78216\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reviews['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHyYnLveFXXE",
        "outputId": "9c2c9a28-ec48-457a-c75e-d82b3139f3cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Really nice prof, he is boring but if you attend his lectures you will do really well, provides good summary notes, overall good person! not bad as everyone says he is :)\n",
            "really nice boring but if you attend his you will do really well provides good summary notes overall good person not bad as everyone says emopos\n",
            "\n",
            "\n",
            "Class is pretty easy. All he grades you on is 4 tests with the 4th being cumulative. The tests are not hard if you pay attention and take notes. He does give out extra credit in the form of &quot;bumps&quot; you get these by being in his &quot;trial&quot; setup, being a student investigator, or being a student senator. Overall fun, easy class. Take him.\n",
            "pretty easy all grades you on tests with the th being cumulative the tests are not hard if you pay attention and take notes does give out extra credit in the form of bumps you get these by being in his trial setup being a student investigator or being a student senator overall fun easy take\n",
            "\n",
            "\n",
            "This professor is amazing.  Very knowledgeable.  Very helpful. He is fair and easy to talk to.  He is always willing to help and is very committed to ensure that you succeed.  All you have to do is commit yourself to learning, be dedicated and responsible.  I definitely recommend you take his class.\n",
            "this amazing very knowledgeable very helpful fair and easy to talk to always willing to help and very committed to ensure that you succeed all you have to do commit yourself to learning be dedicated and responsible definitely recommend you take his\n",
            "\n",
            "\n",
            "Very inspirational, caring and Understanding!\n",
            "very inspirational caring and understanding\n",
            "\n",
            "\n",
            "She's awesome! it is not a complete blow off class but it is pretty damn easy.  Took her for 367.02 barely did any work and still projecting anything from an -A to a B.  Plus she's hot so that's fun.\n",
            "awesome not a complete blow off but pretty damn easy took for barely did any work and still projecting anything from an a to a b plus hot so that fun\n",
            "\n",
            "\n",
            "I took Speech 100 from Ms. Tice back in 1985 at Golden West College in California. It was such a great class that I still remember her name after all these years.\n",
            "took speech from back in at golden west in california was such a great that still remember name after all these years\n",
            "\n",
            "\n",
            "He is very clear when he teaches. Sit in the front row because his tone of voice is very low even though he tries to talk louder. He will give you only two tests and an extra credit essay. If you have questions he will answer them. The tests are tough but make sure you study a week  before the midterm and the final and you will make it.\n",
            "very clear when teaches sit in the front row because his tone of voice very low even though tries to talk louder will give you only two tests and an extra credit essay if you have questions will answer them the tests are tough but make sure you study a week before the midterm and the final and you will make\n",
            "\n",
            "\n",
            "Great professor that you can easily relate too.\n",
            "great that you can easily relate too\n",
            "\n",
            "\n",
            "Great professor. I took this class online. He is very clear on what he requires. This class requires many essays but Mr. Daniels is very helpful with explaining assignments, giving helpful links to help with assignments, and responding to concerns on the forums. This is not an easy course but he is a great teacher to have to get through it.\n",
            "great took this online very clear on what requires this requires many essays but daniels very helpful with explaining assignments giving helpful links to help with assignments and responding to concerns on the forums this not an easy but a great to have to get through\n",
            "\n",
            "\n",
            "she really cares for her students. she expects you to work but will provide all the assistant you need to succeed. love her!\n",
            "really cares for students expects you to work but will provide all the assistant you need to succeed love\n",
            "\n",
            "\n",
            "Prof. Negri was by far the nicest and most caring teacher that I've had at Brookdale. She genuinely cares about her students and wants to see them succeed. She makes a point of going around and making sure that every student fully understands the assignments. I would take her class over again if I could.\n",
            "was by far the nicest and most caring that have had at brookdale genuinely cares about students and wants to see them succeed makes a point of going around and making sure that every student fully understands the assignments would take over again if could\n",
            "\n",
            "\n",
            "She made her class interesting and fun. I highly recommend her.\n",
            "made interesting and fun highly recommend\n",
            "\n",
            "\n",
            "I should preface this review by saying that have never actually taken Mrs. Helfrich as a professor. But I was a full-time student at FCC, and I worked as an Office Assistant for the Social Science and English departments. During that time, I interacted with many of the professors. Mrs. Helfrich was one of the nicest, most caring individuals there.\n",
            "should preface this review by saying that have never actually taken mrs as a but was a full time student at fcc and worked as an office assistant for the social science and english departments during that time interacted with many of the professors mrs was one of the nicest most caring individuals there\n",
            "\n",
            "\n",
            "I liked his class. Kinda.\n",
            "liked his kinda\n",
            "\n",
            "\n",
            "I took his class at the University of Kentucky not Murray State.  He's a really nice professor and you will learn a lot from him.  Although he does give tons of work.  His exams are boarder line impossible but they only count for a very little portion of your final grade.  He never cancels class or lets out early.  Just do all your work and try.\n",
            "took his at the of kentucky not murray state a really nice and you will learn a lot from although does give tons of work his exams are boarder line impossible but they only count for a very little portion of your final grade never cancels or lets out early just do all your work and try\n",
            "\n",
            "\n",
            "I took Ochem 1 and 2 during the summer and got A's in both. How? -He warns that it's a hard class and that you will need to study a lot in the beginning. So, I did.  Read every chapter, did every practice problem on the back.   -He teachers how to write good notes, so I did.  I rewrote them when I got home. -Studied 24/7. No excuses,just results\n",
            "took ochem and during the summer and got a in both how warns that a hard and that you will need to study a lot in the beginning so did read every chapter did every practice problem on the back teachers how to write good notes so did rewrote them when got home studied no excuses just results\n",
            "\n",
            "\n",
            "Amazing!! Loved her! Study and you will do well!\n",
            "amazing loved study and you will do well\n",
            "\n",
            "\n",
            "Hayes is an awesome teacher! she doesn't just go over power point like most professors but adds on to them. so note taking is very important! she will let you know when you need to be paying extra attention and if it will be on the test. the book helps you allot! But don't bother reading anything in the book she doesn't cover. lots of extra stuff.\n",
            "an awesome does not just go over power point like most professors but adds on to them so note taking very important will let you know when you need to be paying extra attention and if will be on the test the helps you allot but do not bother reading anything in the does not cover lots of extra stuff\n",
            "\n",
            "\n",
            "I dont recommend him. Unhelpful, BS HW assignments, & thinks he is the smartest man ever. I wouldn't take him again.\n",
            "dont recommend unhelpful bs assignments thinks the smartest man ever would not take again\n",
            "\n",
            "\n",
            "He is one of the nicest profs & def is willing to help you. However, he sometimes comes across as unclear. If you ask, he will stay on a topic & do his best to explain. Overall, I would definitely take him again. Doing the hw def helps!! Hes an easy grader & tries to give as many pts as possible. I walked away w a B & the class seemed hard to me\n",
            "one of the nicest profs def willing to help you however sometimes comes across as unclear if you ask will stay on a topic do his best to explain overall would definitely take again doing the def helps hes an easy grader tries to give as many pts as possible walked away w a b the seemed hard to\n",
            "\n",
            "\n",
            "I loved her. She was nice, not too hard and fair. She hates when ppl disrupt the class via phones and stupid questions. I took her this winter semester. she gave 2 extra credits,  3 tests and 1 take home. there was one speech. a quiz every class and 4 hws. the class was 3 weeks and awesmome. Great teacher. She is always willing to help out. Just do\n",
            "loved was nice not too hard and fair hates when ppl disrupt the via phones and stupid questions took this winter semester gave extra credits tests and take home there was one speech a every and the was weeks and awesmome great always willing to help out just do\n",
            "\n",
            "\n",
            "Bonnie Million is the sweetest woman! I loved her class. She is interesting and we had really open class discussions. I failed the midterm, though. Study for that but definately take her she is a great teacher.\n",
            "the sweetest woman loved interesting and had really open discussions failed the midterm though study for that but definately take a great\n",
            "\n",
            "\n",
            "I enjoyed taking orgo with McNeil.  She's a really good professor and like everyone said, is very organized.\n",
            "enjoyed taking orgo with a really good and like everyone said very organized\n",
            "\n",
            "\n",
            "Her online class is not hard but also not easy. If you do all your required work and submit your papers in time you are ok. It is definitely not easy to get an A in the papers you write during her class, but she provides a lot of feedback, when sending back the graded paper.\n",
            "online not hard but also not easy if you do all your required work and submit your papers in time you are ok definitely not easy to get an a in the papers you write during but provides a lot of feedback when sending back the graded paper\n",
            "\n",
            "\n",
            "Very straightforward; requires in-depth study of the material.\n",
            "very straightforward requires in depth study of the material\n",
            "\n",
            "\n",
            "Really a good professor. If someone wants help, she is always there to help. Really lucky to have her as a teacher.\n",
            "really a good if someone wants help always there to help really lucky to have as a\n",
            "\n",
            "\n",
            "I would recommed her Good Grief class. She is very helpul, and works with your schedule. She keeps class interesting!\n",
            "would recommed good grief very helpul and works with your schedule keeps interesting\n",
            "\n",
            "\n",
            "Clowe is a really smart guy, he knows exactly what he is talking about. He does a good job at trying to help us understand the material. The homework is rather easy but exams are pretty tough. However his grading scale really helps you out. If you go to class every day and get at least a 60% on every exam you will earn a solid B.\n",
            "a really smart guy knows exactly what talking about does a good job at trying to help us understand the material the homework rather easy but exams are pretty tough however his grading scale really helps you out if you go to every day and get at least a on every exam you will earn a solid b\n",
            "\n",
            "\n",
            "I took Dr. Bielmyer for both 1107, 1108, and then again for Aquat Tox.. She covers a lot of material in one class so either print your power points or bring you computer. If you miss one class you're going to miss alot. Her lab practicals for 1107 and 1108 are pretty tough. Overall I received a B in 1107 and 1108 but an A in Aquat Tox. Take her.\n",
            "took for both and then again for aquat tox covers a lot of material in one so either print your power points or bring you computer if you miss one you are going to miss alot practicals for and are pretty tough overall received a b in and but an a in aquat tox take\n",
            "\n",
            "\n",
            "made spanish sooo easy and fun. he's really funny and sweet.  i never minded going to class. he was great.\n",
            "made spanish so easy and fun really funny and sweet never minded going to was great\n",
            "\n",
            "\n",
            "He rocks, super easy class, very little actual work. Just show up.\n",
            "rocks super easy very little actual work just show up\n",
            "\n",
            "\n",
            "Very good, but difficult instructor. Assigns a great amount of homework. Lectures are great but could be a little more clear. Tests are hard but if you do the homework, you'll be fine.\n",
            "very good but difficult instructor assigns a great amount of homework are great but could be a little more clear tests are hard but if you do the homework you will be fine\n",
            "\n",
            "\n",
            "She was awesome!  And this class was super easy.  She makes lectures interesting and relates them to real life and she's funny too!\n",
            "was awesome and this was super easy makes interesting and relates them to real life and funny too\n",
            "\n",
            "\n",
            "I cannot thank this man enough! He turned my fear/hate of math into something I actually enjoy! He is not easy, but works with students to make sure you understand. Lots of class participation is encouraged. I'm only disappointed I can't take his class every semester!\n",
            "cannot thank this man enough turned my fear hate of math into something actually enjoy not easy but works with students to make sure you understand lots of participation encouraged am only disappointed cannot take his every semester\n",
            "\n",
            "\n",
            "She is fantastic! All around one of the best teachers I've ever had. I took her for Anat. and Phys. 1 & 2. It was hard, but I learned a lot.\n",
            "fantastic all around one of the best teachers have ever had took for anat and phys was hard but learned a lot\n",
            "\n",
            "\n",
            "I did not enjoy this math class at all and math is my favorite subject. She doesn't seem well prepared for teaching. Too many worksheets and repeats a lot of the same material. Would have liked this class to be a bit more challenging.\n",
            "did not enjoy this math at all and math my favorite subject does not seem well prepared for teaching too many worksheets and repeats a lot of the same material would have liked this to be a bit more challenging\n",
            "\n",
            "\n",
            "He has all his old tests online which makes studying for tests WAY EASIER. He also records his lectures and does a live webcam so you can watch onine instead of attending class but he does do pop quizes but gives the people online a chance to go to his office and still do it without penalty. Homework sucks but don't blame him, blame the department\n",
            "has all his old tests online which makes studying for tests way easier also records his and does a live webcam so you can watch onine instead of attending but does do pop quizes but gives the people online a chance to go to his office and still do without penalty homework sucks but do not blame blame the department\n",
            "\n",
            "\n",
            "She is great, very helpful...\n",
            "great very helpful\n",
            "\n",
            "\n",
            "Mr. Panahi is a wonderful and helpful, caring teacher. he really is concerned about your education. he will do anything to help you if you for some reason fall behind. i was very afraid to take 0093 & 1314 but he calmed all my fears. I made a A+ in both. He is the best!\n",
            "a wonderful and helpful caring really concerned about your education will do anything to help you if you for some reason fall behind was very afraid to take but calmed all my fears made a a in both the best\n",
            "\n",
            "\n",
            "Professor Li is excellent! I learned a lot. She made the class fun and challenging. She expects a lot form you and she is fair, but you have to do your part. Do not expect an easy class with her. I highly recommend her if you are accounting major or need to really learn accounting.\n",
            "excellent learned a lot made the fun and challenging expects a lot form you and fair but you have to do your part do not expect an easy with highly recommend if you are accounting major or need to really learn accounting\n",
            "\n",
            "\n",
            "She's nice, Super helpfull, well organized and well teaching.She cares her students a lot.I love to take her class. UR DA BEST TEACHER I'V never had.\n",
            "nice super helpfull well organized and well teaching cares students a lot love to take ur da best v never had\n",
            "\n",
            "\n",
            "An Awesome teache ri would highly recconmend him he really taught verythorough and clear.\n",
            "an awesome teache ri would highly recconmend really taught verythorough and clear\n",
            "\n",
            "\n",
            "Prof Roberts is awesome.  As long as you pay attention in class...you should get an A on every test.  You can tell that he really cares about his students.\n",
            "awesome as long as you pay attention in you should get an a on every test you can tell that really cares about his students\n",
            "\n",
            "\n",
            "Prof. Tonape takes his job seriously. For students with little to none drawing ability, I would recommend that you either put a lot of time into practicing at home (I did four hours a week, but it turned out it wasn't enough) or trust my word and choose not to take it. If you don't have the ability or time, it IS NOT for you!!! Tonape is tough.\n",
            "takes his job seriously for students with little to none drawing ability would recommend that you either put a lot of time into practicing at home did four hours a week but turned out was not enough or trust my word and choose not to take if you do not have the ability or time not for you tough\n",
            "\n",
            "\n",
            "McCormick is extremely easy. If you take this class, there are only 5 quizzes, all open note with about 25 questions. No textbook, no final, no homework. His jokes make the class interesting and the material is interesting as well.\n",
            "mccormick extremely easy if you take this there are only all open note with about questions no textbook no final no homework his jokes make the interesting and the material interesting as well\n",
            "\n",
            "\n",
            "very nice teacher. Not difficult class. I enjoyed the case studies so much and really learned a lot from them.\n",
            "very nice not difficult enjoyed the case studies so much and really learned a lot from them\n",
            "\n",
            "\n",
            "He is helpful.Will help you if you ask questions. Lots of quizzes though. You will do fine as long as you cram the text book. Not much happens in the class.\n",
            "helpful will help you if you ask questions lots of though you will do fine as long as you cram the text not much happens in the\n",
            "\n",
            "\n",
            "The weight training class was a breeze. No textbook, so that was nice. I think we had two take home tests and one in class quiz. Other than that it's up to you to go to class and get your extra workouts done. The instructor was nice and was very good about letting the class know when things were due. I would take her again and/or recommend her.\n",
            "the weight training was a breeze no textbook so that was nice think had two take home tests and one in other than that up to you to go to and get your extra workouts done the instructor was nice and was very good about letting the know when things were due would take again and or recommend\n",
            "\n",
            "\n",
            "Excellent Professor and straight forward class. Lots to read, 2 text books, weekly online quizzes. Gives great reviews for both the mid-term and final.   \n",
            "excellent and straight forward lots to read text weekly online gives great reviews for both the mid term and final\n",
            "\n",
            "\n",
            "She explains topics and questions very clearly and very easy to understand. She is always willing to help students who have question and difficulty. She is just amazing. You should go to the class in order to understand the lessons easily. l'm so glad to had Prof. Bauer. SO NICE AND AMAZING !!!\n",
            "explains topics and questions very clearly and very easy to understand always willing to help students who have question and difficulty just amazing you should go to the in order to understand the lessons easily l m so glad to had so nice and amazing\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for index, row in reviews.head(50).iterrows():\n",
        "    print(row['comment'])\n",
        "    print(row['cleanedComment'])\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWKakPo-v8xK"
      },
      "outputs": [],
      "source": [
        "# dropping rows <= 5\n",
        "# reviews['wordCount'] = reviews[\"cleanedComment\"].str.split().str.len()\n",
        "# reviews[['wordCount', 'cleanedComment']].head(5)\n",
        "\n",
        "# reviews = reviews[reviews['wordCount'] > 5]\n",
        "# reviews.shape\n",
        "reviews = reviews.loc[:, [\"firstName\", \"lastName\", \"comment\", \"cleanedComment\", \"clarityRating\", \"sentiment\", \"professor_id\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fgpG8vr9bdI"
      },
      "source": [
        "# Document Sentiment Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hODBzDigiKcz"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedGroupKFold\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "from sklearn.feature_selection import chi2, SelectPercentile, SelectKBest\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWK7q8fLik7s"
      },
      "outputs": [],
      "source": [
        "def evalPerformance(yp, yt, mode=\"micro\"):\n",
        "    prec_score = precision_score(yt, yp)\n",
        "    rec_score = recall_score(yt, yp)\n",
        "    f1 = f1_score(yt, yp, average=mode)\n",
        "    acc_score = accuracy_score(yt, yp)\n",
        "    conf_m = confusion_matrix(yt, yp)\n",
        "\n",
        "\n",
        "    print(f\"Precision Score: {prec_score*100}\")\n",
        "    print(f\"Recall Score: {rec_score*100}\")\n",
        "    print(\"F1 Score: {0}\".format(f1 * 100))\n",
        "    print(\"Accuracy Score: \" + str(acc_score * 100))\n",
        "    print(conf_m)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LgV4O0LccPE8",
        "outputId": "3bfceaf5-a4bd-49d6-f311-a476c568663a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-39114d19-3eb3-438e-abbf-7b8c5007ff82\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>firstName</th>\n",
              "      <th>lastName</th>\n",
              "      <th>comment</th>\n",
              "      <th>clarityRating</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>professor_id</th>\n",
              "      <th>cleanedComment</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">VGVhY2hlci05Mjc1NTI=</th>\n",
              "      <th>cannot say that methods of grading were unfair however many times requirements for a speech dates and other major things about what was going on were changed in the middle of a if you missed then you may receive a changes like these are not emphasized if you do not have your outline in hand on speech day you get a</th>\n",
              "      <td>.</td>\n",
              "      <td>Ramage</td>\n",
              "      <td>I can't say that her methods of grading were u...</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>does not put up with nonsense or tardiness at all expects respect will teach you the correct way of speaking whether you can adapt to or not what you learn will definitely benefit you in life very organized prompt with returning grades organization worth a lot in a busy schedule</th>\n",
              "      <td>.</td>\n",
              "      <td>Ramage</td>\n",
              "      <td>She does not put up with  nonsense or tardines...</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>maybe the last commenter should have accepted a few lessons from mrs there are at least two misspelled words and at least one misused word in the comments grammar matters mrs a very good learned much more because taught the correct way of doing things</th>\n",
              "      <td>.</td>\n",
              "      <td>Ramage</td>\n",
              "      <td>Maybe the last commenter should have accepted ...</td>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>not relly a speech more of just an outline does not grade you speech just your outline so be orginized took and was not satisfied style more of grading the paper based on grammer more of and english then a speech</th>\n",
              "      <td>.</td>\n",
              "      <td>Ramage</td>\n",
              "      <td>Not relly a speech teacher more of just an out...</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>requires you to print out papers for to grade your speech seems like wants you to fail the if your missing one thing will cause you to not be able to make your speech not the best speech giver either tells the same stories over and over again</th>\n",
              "      <td>.</td>\n",
              "      <td>Ramage</td>\n",
              "      <td>She requires you to print out papers for her t...</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">VGVhY2hlci0xNjIxNTMz</th>\n",
              "      <th>a great in my opinion just really bad hand writing if you want to do well in his like any other uga you must review your in examples and notes the homework are tough but if you follow all his proofs you will be able to solve them most be do poorly because they are not willing to stay after</th>\n",
              "      <td>Ryan</td>\n",
              "      <td>Scott</td>\n",
              "      <td>He is a great teacher in my opinion, just real...</td>\n",
              "      <td>3</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>a hard has test and curves them all</th>\n",
              "      <td>Ryan</td>\n",
              "      <td>Scott</td>\n",
              "      <td>He's a hard teacher. He has 3 test and curves ...</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>teaches the material but does not go back if you missed a day curves the test but the test and homework are super hard go to tutoring get to know your fellow students its small and they are your best bet to better understanding the material loves math you can see think asks questions during that you do not know how to answer</th>\n",
              "      <td>Ryan</td>\n",
              "      <td>Scott</td>\n",
              "      <td>He teaches the material but doesn't go back if...</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tests are hard you need to study and pay attention wants to make sure students understand the concepts there are tests but are curved in the end</th>\n",
              "      <td>Ryan</td>\n",
              "      <td>Scott</td>\n",
              "      <td>Tests are hard. You need to study and pay atte...</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>would not take again if had the choice</th>\n",
              "      <td>Ryan</td>\n",
              "      <td>Scott</td>\n",
              "      <td>I wouldn't take him again if I had the choice.</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>260516 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-39114d19-3eb3-438e-abbf-7b8c5007ff82')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-39114d19-3eb3-438e-abbf-7b8c5007ff82 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-39114d19-3eb3-438e-abbf-7b8c5007ff82');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                                        firstName  \\\n",
              "professor_id         cleanedComment                                                 \n",
              "VGVhY2hlci05Mjc1NTI= cannot say that methods of grading were unfair ...         .   \n",
              "                     does not put up with nonsense or tardiness at a...         .   \n",
              "                     maybe the last commenter should have accepted a...         .   \n",
              "                     not relly a speech more of just an outline does...         .   \n",
              "                     requires you to print out papers for to grade y...         .   \n",
              "...                                                                           ...   \n",
              "VGVhY2hlci0xNjIxNTMz a great in my opinion just really bad hand writ...      Ryan   \n",
              "                     a hard has test and curves them all                     Ryan   \n",
              "                     teaches the material but does not go back if yo...      Ryan   \n",
              "                     tests are hard you need to study and pay attent...      Ryan   \n",
              "                     would not take again if had the choice                  Ryan   \n",
              "\n",
              "                                                                        lastName  \\\n",
              "professor_id         cleanedComment                                                \n",
              "VGVhY2hlci05Mjc1NTI= cannot say that methods of grading were unfair ...   Ramage   \n",
              "                     does not put up with nonsense or tardiness at a...   Ramage   \n",
              "                     maybe the last commenter should have accepted a...   Ramage   \n",
              "                     not relly a speech more of just an outline does...   Ramage   \n",
              "                     requires you to print out papers for to grade y...   Ramage   \n",
              "...                                                                          ...   \n",
              "VGVhY2hlci0xNjIxNTMz a great in my opinion just really bad hand writ...    Scott   \n",
              "                     a hard has test and curves them all                   Scott   \n",
              "                     teaches the material but does not go back if yo...    Scott   \n",
              "                     tests are hard you need to study and pay attent...    Scott   \n",
              "                     would not take again if had the choice                Scott   \n",
              "\n",
              "                                                                                                                   comment  \\\n",
              "professor_id         cleanedComment                                                                                          \n",
              "VGVhY2hlci05Mjc1NTI= cannot say that methods of grading were unfair ...  I can't say that her methods of grading were u...   \n",
              "                     does not put up with nonsense or tardiness at a...  She does not put up with  nonsense or tardines...   \n",
              "                     maybe the last commenter should have accepted a...  Maybe the last commenter should have accepted ...   \n",
              "                     not relly a speech more of just an outline does...  Not relly a speech teacher more of just an out...   \n",
              "                     requires you to print out papers for to grade y...  She requires you to print out papers for her t...   \n",
              "...                                                                                                                    ...   \n",
              "VGVhY2hlci0xNjIxNTMz a great in my opinion just really bad hand writ...  He is a great teacher in my opinion, just real...   \n",
              "                     a hard has test and curves them all                 He's a hard teacher. He has 3 test and curves ...   \n",
              "                     teaches the material but does not go back if yo...  He teaches the material but doesn't go back if...   \n",
              "                     tests are hard you need to study and pay attent...  Tests are hard. You need to study and pay atte...   \n",
              "                     would not take again if had the choice                 I wouldn't take him again if I had the choice.   \n",
              "\n",
              "                                                                         clarityRating  \\\n",
              "professor_id         cleanedComment                                                      \n",
              "VGVhY2hlci05Mjc1NTI= cannot say that methods of grading were unfair ...              1   \n",
              "                     does not put up with nonsense or tardiness at a...              4   \n",
              "                     maybe the last commenter should have accepted a...              4   \n",
              "                     not relly a speech more of just an outline does...              1   \n",
              "                     requires you to print out papers for to grade y...              3   \n",
              "...                                                                                ...   \n",
              "VGVhY2hlci0xNjIxNTMz a great in my opinion just really bad hand writ...              3   \n",
              "                     a hard has test and curves them all                             2   \n",
              "                     teaches the material but does not go back if yo...              3   \n",
              "                     tests are hard you need to study and pay attent...              2   \n",
              "                     would not take again if had the choice                          1   \n",
              "\n",
              "                                                                         sentiment  \n",
              "professor_id         cleanedComment                                                 \n",
              "VGVhY2hlci05Mjc1NTI= cannot say that methods of grading were unfair ...      False  \n",
              "                     does not put up with nonsense or tardiness at a...       True  \n",
              "                     maybe the last commenter should have accepted a...       True  \n",
              "                     not relly a speech more of just an outline does...      False  \n",
              "                     requires you to print out papers for to grade y...      False  \n",
              "...                                                                            ...  \n",
              "VGVhY2hlci0xNjIxNTMz a great in my opinion just really bad hand writ...       True  \n",
              "                     a hard has test and curves them all                     False  \n",
              "                     teaches the material but does not go back if yo...      False  \n",
              "                     tests are hard you need to study and pay attent...      False  \n",
              "                     would not take again if had the choice                  False  \n",
              "\n",
              "[260516 rows x 5 columns]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "review_ids = reviews.groupby(['professor_id', 'cleanedComment']) \n",
        "review_ids.first()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-Z9aUrLZXEO",
        "outputId": "d201f093-39ee-403a-c973-75a645077412"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['VGVhY2hlci0xMDA5OTAz' 'VGVhY2hlci0xMTM0MDI3' 'VGVhY2hlci0xNTY2MTcw'\n",
            " 'VGVhY2hlci0xNjIwMjE4' 'VGVhY2hlci05NjQ4NTY=']\n",
            "0    True\n",
            "1    True\n",
            "2    True\n",
            "3    True\n",
            "4    True\n",
            "Name: sentiment, dtype: bool\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0    really nice boring but if you attend his you w...\n",
              "1    pretty easy all grades you on tests with the t...\n",
              "2    this amazing very knowledgeable very helpful f...\n",
              "3          very inspirational caring and understanding\n",
              "4    awesome not a complete blow off but pretty dam...\n",
              "Name: cleanedComment, dtype: object"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "groups_professor_id_list = np.array(reviews['professor_id'].values)\n",
        "print(groups_professor_id_list[:5])\n",
        "\n",
        "y = reviews['sentiment']\n",
        "print(y.head(5))\n",
        "\n",
        "X = reviews['cleanedComment']\n",
        "X.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGK0_PdnY06y"
      },
      "source": [
        "## Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neJl7wH8iFOq"
      },
      "outputs": [],
      "source": [
        "sent_pipeline = Pipeline([\n",
        "    # (\"vectorizer\", CountVectorizer(ngram_range=(1,2), max_df=0.5)), \n",
        "    (\"vectorizer\", TfidfVectorizer(ngram_range=(1,2), min_df = 7)),\n",
        "    (\"selector\"  , SelectPercentile(score_func=chi2, percentile=30)),\n",
        "    (\"classifier\" , MultinomialNB(alpha=1.0))\n",
        "    # (\"classifer\" , DecisionTreeClassifier(max_depth=5))\n",
        "])\n",
        "\n",
        "# sent_pipeline = Pipeline([\n",
        "#     (\"vectorizer\", TfidfVectorizer(ngram_range=(1,2), min_df=8, stop_words=stopwords)),\n",
        "#     (\"selector\"  , SelectPercentile(score_func=chi2, percentile=30)),\n",
        "#     (\"classifier\" , MultinomialNB(alpha=1.0))\n",
        "# ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Xkfrjo7Zyhv"
      },
      "outputs": [],
      "source": [
        "param_list = {\n",
        "    'vectorizer__ngram_range': [(1, 3)],\n",
        "    'vectorizer__min_df': [6, 7, 8, 9],\n",
        "    'selector__percentile': range(10, 41, 2),\n",
        "    'classifier__alpha': np.arange(0, .5, .05),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZApmZcFYg_N"
      },
      "outputs": [],
      "source": [
        "sgkf = StratifiedGroupKFold(n_splits = 5)\n",
        "random_search = RandomizedSearchCV(sent_pipeline, param_list, scoring='f1_micro', cv=sgkf, n_iter=20, verbose=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut1td3pakWqX"
      },
      "source": [
        "## Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5B4B3KDZRcl",
        "outputId": "e49fdf4a-2527-4e66-f653-ad160dcee4f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "[CV 1/5] END classifier__alpha=0.05, selector__percentile=10, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.929 total time=  25.6s\n",
            "[CV 2/5] END classifier__alpha=0.05, selector__percentile=10, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.931 total time=  25.6s\n",
            "[CV 3/5] END classifier__alpha=0.05, selector__percentile=10, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.931 total time=  25.2s\n",
            "[CV 4/5] END classifier__alpha=0.05, selector__percentile=10, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.932 total time=  25.7s\n",
            "[CV 5/5] END classifier__alpha=0.05, selector__percentile=10, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.929 total time=  25.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py:557: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
            "  % _ALPHA_MIN\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 1/5] END classifier__alpha=0.0, selector__percentile=40, vectorizer__min_df=9, vectorizer__ngram_range=(1, 3);, score=0.928 total time=  25.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py:557: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
            "  % _ALPHA_MIN\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 2/5] END classifier__alpha=0.0, selector__percentile=40, vectorizer__min_df=9, vectorizer__ngram_range=(1, 3);, score=0.928 total time=  25.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py:557: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
            "  % _ALPHA_MIN\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 3/5] END classifier__alpha=0.0, selector__percentile=40, vectorizer__min_df=9, vectorizer__ngram_range=(1, 3);, score=0.927 total time=  25.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py:557: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
            "  % _ALPHA_MIN\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 4/5] END classifier__alpha=0.0, selector__percentile=40, vectorizer__min_df=9, vectorizer__ngram_range=(1, 3);, score=0.929 total time=  25.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py:557: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
            "  % _ALPHA_MIN\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 5/5] END classifier__alpha=0.0, selector__percentile=40, vectorizer__min_df=9, vectorizer__ngram_range=(1, 3);, score=0.927 total time=  25.4s\n",
            "[CV 1/5] END classifier__alpha=0.2, selector__percentile=24, vectorizer__min_df=6, vectorizer__ngram_range=(1, 3);, score=0.934 total time=  25.9s\n",
            "[CV 2/5] END classifier__alpha=0.2, selector__percentile=24, vectorizer__min_df=6, vectorizer__ngram_range=(1, 3);, score=0.935 total time=  25.6s\n",
            "[CV 3/5] END classifier__alpha=0.2, selector__percentile=24, vectorizer__min_df=6, vectorizer__ngram_range=(1, 3);, score=0.934 total time=  25.6s\n",
            "[CV 4/5] END classifier__alpha=0.2, selector__percentile=24, vectorizer__min_df=6, vectorizer__ngram_range=(1, 3);, score=0.935 total time=  25.9s\n",
            "[CV 5/5] END classifier__alpha=0.2, selector__percentile=24, vectorizer__min_df=6, vectorizer__ngram_range=(1, 3);, score=0.934 total time=  25.9s\n",
            "[CV 1/5] END classifier__alpha=0.4, selector__percentile=30, vectorizer__min_df=6, vectorizer__ngram_range=(1, 3);, score=0.934 total time=  29.4s\n",
            "[CV 2/5] END classifier__alpha=0.4, selector__percentile=30, vectorizer__min_df=6, vectorizer__ngram_range=(1, 3);, score=0.936 total time=  26.3s\n",
            "[CV 3/5] END classifier__alpha=0.4, selector__percentile=30, vectorizer__min_df=6, vectorizer__ngram_range=(1, 3);, score=0.935 total time=  26.7s\n",
            "[CV 4/5] END classifier__alpha=0.4, selector__percentile=30, vectorizer__min_df=6, vectorizer__ngram_range=(1, 3);, score=0.936 total time=  26.8s\n",
            "[CV 5/5] END classifier__alpha=0.4, selector__percentile=30, vectorizer__min_df=6, vectorizer__ngram_range=(1, 3);, score=0.934 total time=  26.7s\n",
            "[CV 1/5] END classifier__alpha=0.25, selector__percentile=16, vectorizer__min_df=6, vectorizer__ngram_range=(1, 3);, score=0.933 total time=  26.9s\n",
            "[CV 2/5] END classifier__alpha=0.25, selector__percentile=16, vectorizer__min_df=6, vectorizer__ngram_range=(1, 3);, score=0.934 total time=  26.5s\n",
            "[CV 3/5] END classifier__alpha=0.25, selector__percentile=16, vectorizer__min_df=6, vectorizer__ngram_range=(1, 3);, score=0.933 total time=  26.8s\n",
            "[CV 4/5] END classifier__alpha=0.25, selector__percentile=16, vectorizer__min_df=6, vectorizer__ngram_range=(1, 3);, score=0.935 total time=  26.9s\n",
            "[CV 5/5] END classifier__alpha=0.25, selector__percentile=16, vectorizer__min_df=6, vectorizer__ngram_range=(1, 3);, score=0.932 total time=  26.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py:557: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
            "  % _ALPHA_MIN\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 1/5] END classifier__alpha=0.0, selector__percentile=12, vectorizer__min_df=7, vectorizer__ngram_range=(1, 3);, score=0.927 total time=  26.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py:557: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
            "  % _ALPHA_MIN\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 2/5] END classifier__alpha=0.0, selector__percentile=12, vectorizer__min_df=7, vectorizer__ngram_range=(1, 3);, score=0.928 total time=  26.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py:557: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
            "  % _ALPHA_MIN\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 3/5] END classifier__alpha=0.0, selector__percentile=12, vectorizer__min_df=7, vectorizer__ngram_range=(1, 3);, score=0.927 total time=  26.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py:557: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
            "  % _ALPHA_MIN\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 4/5] END classifier__alpha=0.0, selector__percentile=12, vectorizer__min_df=7, vectorizer__ngram_range=(1, 3);, score=0.929 total time=  27.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py:557: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
            "  % _ALPHA_MIN\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 5/5] END classifier__alpha=0.0, selector__percentile=12, vectorizer__min_df=7, vectorizer__ngram_range=(1, 3);, score=0.927 total time=  26.6s\n",
            "[CV 1/5] END classifier__alpha=0.25, selector__percentile=10, vectorizer__min_df=9, vectorizer__ngram_range=(1, 3);, score=0.928 total time=  26.1s\n",
            "[CV 2/5] END classifier__alpha=0.25, selector__percentile=10, vectorizer__min_df=9, vectorizer__ngram_range=(1, 3);, score=0.930 total time=  26.4s\n",
            "[CV 3/5] END classifier__alpha=0.25, selector__percentile=10, vectorizer__min_df=9, vectorizer__ngram_range=(1, 3);, score=0.930 total time=  26.1s\n",
            "[CV 4/5] END classifier__alpha=0.25, selector__percentile=10, vectorizer__min_df=9, vectorizer__ngram_range=(1, 3);, score=0.930 total time=  26.6s\n",
            "[CV 5/5] END classifier__alpha=0.25, selector__percentile=10, vectorizer__min_df=9, vectorizer__ngram_range=(1, 3);, score=0.928 total time=  27.4s\n",
            "[CV 1/5] END classifier__alpha=0.25, selector__percentile=32, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.934 total time=  27.3s\n",
            "[CV 2/5] END classifier__alpha=0.25, selector__percentile=32, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.936 total time=  27.5s\n",
            "[CV 3/5] END classifier__alpha=0.25, selector__percentile=32, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.934 total time=  27.4s\n",
            "[CV 4/5] END classifier__alpha=0.25, selector__percentile=32, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.936 total time=  27.3s\n",
            "[CV 5/5] END classifier__alpha=0.25, selector__percentile=32, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.934 total time=  27.3s\n",
            "[CV 1/5] END classifier__alpha=0.45, selector__percentile=34, vectorizer__min_df=9, vectorizer__ngram_range=(1, 3);, score=0.934 total time=  26.7s\n",
            "[CV 2/5] END classifier__alpha=0.45, selector__percentile=34, vectorizer__min_df=9, vectorizer__ngram_range=(1, 3);, score=0.935 total time=  26.7s\n",
            "[CV 3/5] END classifier__alpha=0.45, selector__percentile=34, vectorizer__min_df=9, vectorizer__ngram_range=(1, 3);, score=0.934 total time=  26.8s\n",
            "[CV 4/5] END classifier__alpha=0.45, selector__percentile=34, vectorizer__min_df=9, vectorizer__ngram_range=(1, 3);, score=0.936 total time=  27.1s\n",
            "[CV 5/5] END classifier__alpha=0.45, selector__percentile=34, vectorizer__min_df=9, vectorizer__ngram_range=(1, 3);, score=0.933 total time=  26.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py:557: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
            "  % _ALPHA_MIN\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 1/5] END classifier__alpha=0.0, selector__percentile=28, vectorizer__min_df=7, vectorizer__ngram_range=(1, 3);, score=0.929 total time=  27.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py:557: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
            "  % _ALPHA_MIN\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 2/5] END classifier__alpha=0.0, selector__percentile=28, vectorizer__min_df=7, vectorizer__ngram_range=(1, 3);, score=0.930 total time=  26.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py:557: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
            "  % _ALPHA_MIN\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 3/5] END classifier__alpha=0.0, selector__percentile=28, vectorizer__min_df=7, vectorizer__ngram_range=(1, 3);, score=0.928 total time=  28.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py:557: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
            "  % _ALPHA_MIN\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 4/5] END classifier__alpha=0.0, selector__percentile=28, vectorizer__min_df=7, vectorizer__ngram_range=(1, 3);, score=0.930 total time=  27.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py:557: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
            "  % _ALPHA_MIN\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 5/5] END classifier__alpha=0.0, selector__percentile=28, vectorizer__min_df=7, vectorizer__ngram_range=(1, 3);, score=0.928 total time=  27.6s\n",
            "[CV 1/5] END classifier__alpha=0.1, selector__percentile=36, vectorizer__min_df=6, vectorizer__ngram_range=(1, 3);, score=0.935 total time=  27.8s\n",
            "[CV 2/5] END classifier__alpha=0.1, selector__percentile=36, vectorizer__min_df=6, vectorizer__ngram_range=(1, 3);, score=0.936 total time=  28.3s\n",
            "[CV 3/5] END classifier__alpha=0.1, selector__percentile=36, vectorizer__min_df=6, vectorizer__ngram_range=(1, 3);, score=0.935 total time=  28.3s\n",
            "[CV 4/5] END classifier__alpha=0.1, selector__percentile=36, vectorizer__min_df=6, vectorizer__ngram_range=(1, 3);, score=0.936 total time=  28.5s\n",
            "[CV 5/5] END classifier__alpha=0.1, selector__percentile=36, vectorizer__min_df=6, vectorizer__ngram_range=(1, 3);, score=0.935 total time=  28.0s\n",
            "[CV 1/5] END classifier__alpha=0.15000000000000002, selector__percentile=38, vectorizer__min_df=9, vectorizer__ngram_range=(1, 3);, score=0.935 total time=  28.0s\n",
            "[CV 2/5] END classifier__alpha=0.15000000000000002, selector__percentile=38, vectorizer__min_df=9, vectorizer__ngram_range=(1, 3);, score=0.936 total time=  27.6s\n",
            "[CV 3/5] END classifier__alpha=0.15000000000000002, selector__percentile=38, vectorizer__min_df=9, vectorizer__ngram_range=(1, 3);, score=0.934 total time=  27.5s\n",
            "[CV 4/5] END classifier__alpha=0.15000000000000002, selector__percentile=38, vectorizer__min_df=9, vectorizer__ngram_range=(1, 3);, score=0.936 total time=  28.0s\n",
            "[CV 5/5] END classifier__alpha=0.15000000000000002, selector__percentile=38, vectorizer__min_df=9, vectorizer__ngram_range=(1, 3);, score=0.934 total time=  27.9s\n",
            "[CV 1/5] END classifier__alpha=0.15000000000000002, selector__percentile=32, vectorizer__min_df=7, vectorizer__ngram_range=(1, 3);, score=0.934 total time=  28.4s\n",
            "[CV 2/5] END classifier__alpha=0.15000000000000002, selector__percentile=32, vectorizer__min_df=7, vectorizer__ngram_range=(1, 3);, score=0.936 total time=  28.4s\n",
            "[CV 3/5] END classifier__alpha=0.15000000000000002, selector__percentile=32, vectorizer__min_df=7, vectorizer__ngram_range=(1, 3);, score=0.935 total time=  28.4s\n",
            "[CV 4/5] END classifier__alpha=0.15000000000000002, selector__percentile=32, vectorizer__min_df=7, vectorizer__ngram_range=(1, 3);, score=0.937 total time=  28.8s\n",
            "[CV 5/5] END classifier__alpha=0.15000000000000002, selector__percentile=32, vectorizer__min_df=7, vectorizer__ngram_range=(1, 3);, score=0.935 total time=  28.6s\n",
            "[CV 1/5] END classifier__alpha=0.2, selector__percentile=10, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.929 total time=  28.1s\n",
            "[CV 2/5] END classifier__alpha=0.2, selector__percentile=10, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.930 total time=  28.1s\n",
            "[CV 3/5] END classifier__alpha=0.2, selector__percentile=10, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.930 total time=  26.6s\n",
            "[CV 4/5] END classifier__alpha=0.2, selector__percentile=10, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.931 total time=  26.8s\n",
            "[CV 5/5] END classifier__alpha=0.2, selector__percentile=10, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.929 total time=  26.8s\n",
            "[CV 1/5] END classifier__alpha=0.05, selector__percentile=22, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.934 total time=  26.4s\n",
            "[CV 2/5] END classifier__alpha=0.05, selector__percentile=22, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.935 total time=  26.3s\n",
            "[CV 3/5] END classifier__alpha=0.05, selector__percentile=22, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.934 total time=  26.3s\n",
            "[CV 4/5] END classifier__alpha=0.05, selector__percentile=22, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.935 total time=  26.0s\n",
            "[CV 5/5] END classifier__alpha=0.05, selector__percentile=22, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.933 total time=  26.1s\n",
            "[CV 1/5] END classifier__alpha=0.35000000000000003, selector__percentile=18, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.932 total time=  26.4s\n",
            "[CV 2/5] END classifier__alpha=0.35000000000000003, selector__percentile=18, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.933 total time=  26.2s\n",
            "[CV 3/5] END classifier__alpha=0.35000000000000003, selector__percentile=18, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.933 total time=  26.1s\n",
            "[CV 4/5] END classifier__alpha=0.35000000000000003, selector__percentile=18, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.934 total time=  26.2s\n",
            "[CV 5/5] END classifier__alpha=0.35000000000000003, selector__percentile=18, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.932 total time=  26.5s\n",
            "[CV 1/5] END classifier__alpha=0.15000000000000002, selector__percentile=22, vectorizer__min_df=6, vectorizer__ngram_range=(1, 3);, score=0.934 total time=  26.6s\n",
            "[CV 2/5] END classifier__alpha=0.15000000000000002, selector__percentile=22, vectorizer__min_df=6, vectorizer__ngram_range=(1, 3);, score=0.935 total time=  26.1s\n",
            "[CV 3/5] END classifier__alpha=0.15000000000000002, selector__percentile=22, vectorizer__min_df=6, vectorizer__ngram_range=(1, 3);, score=0.934 total time=  26.2s\n",
            "[CV 4/5] END classifier__alpha=0.15000000000000002, selector__percentile=22, vectorizer__min_df=6, vectorizer__ngram_range=(1, 3);, score=0.935 total time=  26.1s\n",
            "[CV 5/5] END classifier__alpha=0.15000000000000002, selector__percentile=22, vectorizer__min_df=6, vectorizer__ngram_range=(1, 3);, score=0.934 total time=  25.9s\n",
            "[CV 1/5] END classifier__alpha=0.05, selector__percentile=18, vectorizer__min_df=7, vectorizer__ngram_range=(1, 3);, score=0.934 total time=  25.4s\n",
            "[CV 2/5] END classifier__alpha=0.05, selector__percentile=18, vectorizer__min_df=7, vectorizer__ngram_range=(1, 3);, score=0.934 total time=  25.7s\n",
            "[CV 3/5] END classifier__alpha=0.05, selector__percentile=18, vectorizer__min_df=7, vectorizer__ngram_range=(1, 3);, score=0.933 total time=  25.9s\n",
            "[CV 4/5] END classifier__alpha=0.05, selector__percentile=18, vectorizer__min_df=7, vectorizer__ngram_range=(1, 3);, score=0.935 total time=  25.7s\n",
            "[CV 5/5] END classifier__alpha=0.05, selector__percentile=18, vectorizer__min_df=7, vectorizer__ngram_range=(1, 3);, score=0.933 total time=  26.7s\n",
            "[CV 1/5] END classifier__alpha=0.4, selector__percentile=14, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.930 total time=  26.0s\n",
            "[CV 2/5] END classifier__alpha=0.4, selector__percentile=14, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.932 total time=  25.7s\n",
            "[CV 3/5] END classifier__alpha=0.4, selector__percentile=14, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.931 total time=  26.0s\n",
            "[CV 4/5] END classifier__alpha=0.4, selector__percentile=14, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.933 total time=  25.8s\n",
            "[CV 5/5] END classifier__alpha=0.4, selector__percentile=14, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.931 total time=  25.4s\n",
            "[CV 1/5] END classifier__alpha=0.1, selector__percentile=20, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.933 total time=  25.3s\n",
            "[CV 2/5] END classifier__alpha=0.1, selector__percentile=20, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.935 total time=  25.4s\n",
            "[CV 3/5] END classifier__alpha=0.1, selector__percentile=20, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.933 total time=  25.6s\n",
            "[CV 4/5] END classifier__alpha=0.1, selector__percentile=20, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.935 total time=  25.4s\n",
            "[CV 5/5] END classifier__alpha=0.1, selector__percentile=20, vectorizer__min_df=8, vectorizer__ngram_range=(1, 3);, score=0.933 total time=  25.8s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=StratifiedGroupKFold(n_splits=5, random_state=None, shuffle=False),\n",
              "                   estimator=Pipeline(steps=[('vectorizer',\n",
              "                                              TfidfVectorizer(min_df=7,\n",
              "                                                              ngram_range=(1,\n",
              "                                                                           2))),\n",
              "                                             ('selector',\n",
              "                                              SelectPercentile(percentile=30,\n",
              "                                                               score_func=<function chi2 at 0x7fba76231cb0>)),\n",
              "                                             ('classifier', MultinomialNB())]),\n",
              "                   n_iter=20,\n",
              "                   param_distributions={'classifier__alpha': array([0.  , 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45]),\n",
              "                                        'selector__percentile': range(10, 41, 2),\n",
              "                                        'vectorizer__min_df': [6, 7, 8, 9],\n",
              "                                        'vectorizer__ngram_range': [(1, 3)]},\n",
              "                   scoring='f1_micro', verbose=3)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "random_search.fit(X, y, groups=groups_professor_id_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4xgujDtOAnH"
      },
      "source": [
        "## Estimator Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bs2o0LnM9pN-",
        "outputId": "965fedb6-e27d-4ee2-e37c-42a6d874a97a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'vectorizer__ngram_range': (1, 3), 'vectorizer__min_df': 6, 'selector__percentile': 36, 'classifier__alpha': 0.1}\n",
            "Pipeline(steps=[('vectorizer', TfidfVectorizer(min_df=6, ngram_range=(1, 3))),\n",
            "                ('selector',\n",
            "                 SelectPercentile(percentile=36,\n",
            "                                  score_func=<function chi2 at 0x7fba76231cb0>)),\n",
            "                ('classifier', MultinomialNB(alpha=0.1))])\n"
          ]
        }
      ],
      "source": [
        "print(random_search.best_params_)\n",
        "print(random_search.best_estimator_)\n",
        "best_estimator = random_search.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "v7SIxo7XlYwy"
      },
      "outputs": [],
      "source": [
        "# best_estimator = Pipeline([\n",
        "#     (\"vectorizer\", TfidfVectorizer(ngram_range=(1,2), min_df=8, stop_words=stopwords)),\n",
        "#     (\"selector\"  , SelectPercentile(score_func=chi2, percentile=30)),\n",
        "#     (\"classifer\" , MultinomialNB())\n",
        "# ])\n",
        "# best_estimator.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJURPM6lkxvG"
      },
      "source": [
        "# Validating (or Testing???)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "grbfpSHzj4vb"
      },
      "outputs": [],
      "source": [
        "test_reviews = pd.read_csv(\"/content/drive/MyDrive/RMP/scraped_comments.csv\").sample(n=120000, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DNKBABCDkInt",
        "outputId": "2aa3d9be-1ca9-499f-d4c1-9e768dccd6e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape before dropping: (120000, 16)\n",
            "Shape after dropping: (106780, 18)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ],
      "source": [
        "print(\"Shape before dropping:\", test_reviews.shape)\n",
        "test_reviews.drop_duplicates(subset=\"comment_id\", keep=\"first\", inplace=True)\n",
        "\n",
        "# drop rows containing only \"No Comments\" (default value assigned by RMP to a review that didn't enter a comment)\n",
        "test_reviews = test_reviews[test_reviews[\"comment\"] != \"No Comments\"]\n",
        "\n",
        "# drop rows containing NaN comment\n",
        "test_reviews.dropna(subset=[\"comment\"], inplace=True)\n",
        "\n",
        "# fill null names with empty string\n",
        "test_reviews['firstName'].fillna('', inplace=True)\n",
        "test_reviews['lastName'].fillna('', inplace=True)\n",
        "\n",
        "# Dropping test_reviews with qualityRating == 3\n",
        "test_reviews['qualityRating'] = (test_reviews['helpfulRating']+test_reviews['clarityRating'])/2.0\n",
        "test_reviews = test_reviews[test_reviews[\"qualityRating\"] != 3.0]\n",
        "test_reviews[\"sentiment\"] = test_reviews[\"qualityRating\"] > 3.0\n",
        "\n",
        "print(\"Shape after dropping:\", test_reviews.shape)\n",
        "test_reviews.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89kZU5EskZst"
      },
      "outputs": [],
      "source": [
        "comments_proper = []\n",
        "\n",
        "comments_proper = preprocess_pipeline(test_reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sL639IqRkpVx"
      },
      "outputs": [],
      "source": [
        "sent_predA = best_estimator.predict(comments_proper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtYNIKMUmzyM"
      },
      "outputs": [],
      "source": [
        "test_reviews.reset_index(inplace=True, drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlGQQhFDafI_"
      },
      "source": [
        "## Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-q4csk0k8D0"
      },
      "outputs": [],
      "source": [
        "evalPerformance(sent_predA, test_reviews['sentiment'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amqFgxlce_bB"
      },
      "outputs": [],
      "source": [
        "right = sum(a == b for a, b in zip(sent_predA, test_reviews['sentiment']))\n",
        "right / len(sent_predA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzl-f7BvdTIS"
      },
      "source": [
        "# Possible Improvements\n",
        "* Could engineer new features using words that are capitalized in the review\n",
        "* Remove reviews that're not english"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "RMP Parameter Tuning",
      "toc_visible": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}