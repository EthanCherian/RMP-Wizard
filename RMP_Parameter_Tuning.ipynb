{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RMP Parameter Tuning",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Setup\n",
        "Basic steps, because there are domain specific problems to account for later"
      ],
      "metadata": {
        "id": "gQV3OxqpZG_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import spacy\n",
        "import pkg_resources\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from collections import Counter\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import words, wordnet, brown"
      ],
      "metadata": {
        "id": "k7nhxQ8_ZI86"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install symspellpy\n",
        "\n",
        "from spacy.cli import download\n",
        "download('en_core_web_md')\n",
        "nltk.download('words')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "6jDqCGD3s7Ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from symspellpy import SymSpell, Verbosity"
      ],
      "metadata": {
        "id": "MnPL-zIAs8Ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UMSveQ7KZ2R_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = pd.read_csv(\"/content/drive/MyDrive/RMP/reviews_filtered.csv\")\n",
        "# reviews = pd.read_csv(\"/content/drive/MyDrive/RMP/scraped_comments_with_professor.csv\")\n",
        "# .sample(n=120000, random_state=1)\n",
        "# reviews[['comment_id', 'firstName', 'lastName', 'prof_class', 'comment', 'clarityRating', 'helpfulRating']].to_csv(\"/content/drive/MyDrive/RMP/scraped_comments_sample.csv\")"
      ],
      "metadata": {
        "id": "yLicijJQZLzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews.head()"
      ],
      "metadata": {
        "id": "9vGcWkCVlghQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Duplicates and Nulls"
      ],
      "metadata": {
        "id": "tGS9yvAMag7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape before dropping:\", reviews.shape)\n",
        "reviews.drop_duplicates(subset=\"comment_id\", keep=\"first\", inplace=True)\n",
        "\n",
        "# drop rows containing only \"No Comments\" (default value assigned by RMP to a review that didn't enter a comment)\n",
        "reviews = reviews[reviews[\"comment\"] != \"No Comments\"]\n",
        "\n",
        "# drop rows containing NaN comment\n",
        "reviews.dropna(subset=[\"comment\"], inplace=True)\n",
        "\n",
        "# fill null names with empty string\n",
        "reviews['firstName'].fillna('', inplace=True)\n",
        "reviews['lastName'].fillna('', inplace=True)\n",
        "\n",
        "# Dropping reviews with clarityRating == 3\n",
        "reviews = reviews[reviews[\"clarityRating\"] != 3]\n",
        "reviews[\"sentiment\"] = reviews[\"clarityRating\"].apply(lambda x: 1 if x > 2.5 else 0)\n",
        "\n",
        "print(\"Shape after dropping:\", reviews.shape)\n",
        "reviews.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "CLUgn5sbZ_1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing Urls, Phone Numbers, and Emails"
      ],
      "metadata": {
        "id": "yDFaJC7Fe4nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_urls(text):\n",
        "    return re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
        "\n",
        "def remove_phones(text):\n",
        "    return re.sub(r'\\d{3}-\\d{3}-\\d{4}', ' ', text)\n",
        "\n",
        "def remove_emails(text):\n",
        "    return re.sub(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', ' ', text)\n",
        "\n",
        "print(remove_urls('Hey! Check out this link: www.somelink.com'))\n",
        "print(remove_phones(\"Hey! Check out this phone number: 742-457-0417\"))\n",
        "print(remove_emails(\"Hey! Check out this email address: nooneuses@yahoo.com\"))"
      ],
      "metadata": {
        "id": "yzZSR-eXe9Cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Html Artifacts"
      ],
      "metadata": {
        "id": "ZDTjxLjqnu-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Convert html entites of quotes -> \"'\" to normalize\n",
        "def remove_html_entities(text):\n",
        "  text = re.sub('&[0-9a-zA-Z#]+;', ' ', text)\n",
        "  return re.sub('&#63;?', '', text)\n",
        "\n",
        "def remove_html_tags(text):\n",
        "  return re.sub('<.{1,6}?>', ' ', text)\n",
        "\n",
        "text = \"This professor is such an easy &quot;A&quot;, why are y'all struggling &#63;&#63;&#63 </div>\"\n",
        "print(remove_html_entities(text))\n",
        "print(remove_html_tags(text))"
      ],
      "metadata": {
        "id": "BEmTjY8mnzaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Emoticon Conversion to Words"
      ],
      "metadata": {
        "id": "BxBEJJO9a-Jo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Emoticon Mapping\n",
        "EMOTICONS = {\n",
        "    u\":‑)\":\"emopos\",\n",
        "    u\":-))\":\"emopos\",\n",
        "    u\":-)))\":\"emopos\",\n",
        "    u\":)\":\"emopos\",\n",
        "    u\":))\":\"emopos\",\n",
        "    u\":)))\":\"emopos\",\n",
        "    u\":-]\":\"emopos\",\n",
        "    u\":]\":\"emopos\",\n",
        "    u\":-3\":\"emopos\",\n",
        "    u\":3\":\"emopos\",\n",
        "    u\":->\":\"emopos\",\n",
        "    u\":>\":\"emopos\",\n",
        "    u\"8-)\":\"emopos\",\n",
        "    u\":-}\":\"emopos\",\n",
        "    u\":}\":\"emopos\",\n",
        "    u\":-)\":\"emopos\",\n",
        "    u\":c)\":\"emopos\",\n",
        "    u\":^)\":\"emopos\",\n",
        "    u\"=]\":\"emopos\",\n",
        "    u\"=)\":\"emopos\",\n",
        "    u\":‑D\":\"emopos\",\n",
        "    u\":D\":\"emopos\",\n",
        "    u\"8‑D\":\"emopos\",\n",
        "    u\"8D\":\"emopos\",\n",
        "    u\"X‑D\":\"emopos\",\n",
        "    u\"XD\":\"emopos\",\n",
        "    u\"=D\":\"emopos\",\n",
        "    u\"=3\":\"emopos\",\n",
        "    u\"B^D\":\"emopos\",\n",
        "    u\":-))\":\"emopos\",\n",
        "    u\":-(\":\"emoneg\",\n",
        "    u\":‑(\":\"emoneg\",\n",
        "    u\":(\":\"emoneg\",\n",
        "    u\":‑c\":\"emoneg\",\n",
        "    u\":c\":\"emoneg\",\n",
        "    u\":‑<\":\"emoneg\",\n",
        "    u\":<\":\"emoneg\",\n",
        "    u\":‑[\":\"emoneg\",\n",
        "    u\":[\":\"emoneg\",\n",
        "    u\":-||\":\"emoneg\",\n",
        "    u\">:[\":\"emoneg\",\n",
        "    u\":{\":\"emoneg\",\n",
        "    u\">:(\":\"emoneg\",\n",
        "    u\":'‑(\":\"emoneg\",\n",
        "    u\":'(\":\"emoneg\",\n",
        "    u\":'‑)\":\"emopos\",\n",
        "    u\":')\":\"emopos\",\n",
        "    u\"D‑':\":\"emoneg\",\n",
        "    u\"D:<\":\"emoneg\",\n",
        "    u\"D:\":\"emoneg\",\n",
        "    u\"D8\":\"emoneg\",\n",
        "    u\"D;\":\"emoneg\",\n",
        "    u\"D=\":\"emoneg\",\n",
        "    u\"DX\":\"emoneg\",\n",
        "    u\";‑)\":\"emopos\",\n",
        "    u\";)\":\"emopos\",\n",
        "    u\"*-)\":\"emopos\",\n",
        "    u\"*)\":\"emopos\",\n",
        "    u\";‑]\":\"emopos\",\n",
        "    u\";]\":\"emopos\",\n",
        "    u\";^)\":\"emopos\",\n",
        "    u\":‑,\":\"emopos\",\n",
        "    u\";D\":\"emopos\",\n",
        "    u\":‑P\":\"emopos\",\n",
        "    u\":P\":\"emopos\",\n",
        "    u\"X‑P\":\"emopos\",\n",
        "    u\"XP\":\"emopos\",\n",
        "    u\":‑Þ\":\"emopos\",\n",
        "    u\":Þ\":\"emopos\",\n",
        "    u\"=p\":\"emopos\",\n",
        "    u\":‑/\":\"emoneg\",\n",
        "    u\":/\":\"emoneg\",\n",
        "    u\":-[.]\":\"emoneg\",\n",
        "    u\">:[(\\)]\":\"emoneg\",\n",
        "    u\">:/\":\"emoneg\",\n",
        "    u\":[(\\)]\":\"emoneg\",\n",
        "    u\"=/\":\"emoneg\",\n",
        "    u\"=[(\\)]\":\"emoneg\",\n",
        "    u\":L\":\"emoneg\",\n",
        "    u\"=L\":\"emoneg\",\n",
        "    u\":‑|\":\"emoneg\",\n",
        "    u\":|\":\"emoneg\",\n",
        "    u\"O:‑)\":\"emopos\",\n",
        "    u\"O:)\":\"emopos\",\n",
        "    u\"0:‑3\":\"emopos\",\n",
        "    u\"0:3\":\"emopos\",\n",
        "    u\"0:‑)\":\"emopos\",\n",
        "    u\"0:)\":\"emopos\",\n",
        "    u\":‑b\":\"emopos\",\n",
        "    u\"(>_<)\":\"emoneg\",\n",
        "    u\"(>_<)>\":\"emoneg\",\n",
        "    u\"^_^\":\"emopos\",\n",
        "    u\"(^_^)/\":\"emopos\",\n",
        "    u\"(^O^)／\":\"emopos\",\n",
        "    u\"(^o^)／\":\"emopos\",\n",
        "    u\"('_')\":\"emoneg\",\n",
        "    u\"(/_;)\":\"emoneg\",\n",
        "    u\"(T_T) (;_;)\":\"emoneg\",\n",
        "    u\"(;_;\":\"emoneg\",\n",
        "    u\"(;_:)\":\"emoneg\",\n",
        "    u\"(;O;)\":\"emoneg\",\n",
        "    u\"(:_;)\":\"emoneg\",\n",
        "    u\"(ToT)\":\"emoneg\",\n",
        "    u\";_;\":\"emoneg\",\n",
        "    u\";-;\":\"emoneg\",\n",
        "    u\";n;\":\"emoneg\",\n",
        "    u\"Q.Q\":\"emoneg\",\n",
        "    u\"T.T\":\"emoneg\",\n",
        "    u\"Q_Q\":\"emoneg\",\n",
        "    u\"(-.-)\":\"emopos\",\n",
        "    u\"(-_-)\":\"emopos\",\n",
        "    u\"(；一_一)\":\"emopos\",\n",
        "    u\"(=_=)\":\"emoneg\",\n",
        "    u\"^m^\":\"emopos\",\n",
        "    u\">^_^<\":\"emopos\",\n",
        "    u\"<^!^>\":\"emopos\",\n",
        "    u\"^/^\":\"emopos\",\n",
        "    u\"（*^_^*）\" :\"emopos\",\n",
        "    u\"(^<^) (^.^)\":\"emopos\",\n",
        "    u\"(^^)\":\"emopos\",\n",
        "    u\"(^.^)\":\"emopos\",\n",
        "    u\"(^_^.)\":\"emopos\",\n",
        "    u\"(^_^)\":\"emopos\",\n",
        "    u\"(^^)\":\"emopos\",\n",
        "    u\"(^J^)\":\"emopos\",\n",
        "    u\"(*^.^*)\":\"emopos\",\n",
        "    u\"(^—^）\":\"emopos\",\n",
        "    u\"(#^.^#)\":\"emopos\",\n",
        "    u\"(*^0^*)\":\"emopos\",\n",
        "    u\"(*^^)v\":\"emopos\",\n",
        "    u\"(^_^)v\":\"emopos\",\n",
        "    u'(-\"-)':\"emoneg\",\n",
        "    u\"(ーー;)\":\"emoneg\",\n",
        "    u\"(＾ｖ＾)\":\"emopos\",\n",
        "    u\"(＾ｕ＾)\":\"emopos\",\n",
        "    u\"(^)o(^)\":\"emopos\",\n",
        "    u\"(^O^)\":\"emopos\",\n",
        "    u\"(^o^)\":\"emopos\",\n",
        "    u\")^o^(\":\"emopos\",\n",
        "    u\":O o_O\":\"emoneg\",\n",
        "    u\"o_0\":\"emoneg\",\n",
        "    u\"o.O\":\"emoneg\",\n",
        "    u\"(o.o)\":\"emoneg\",\n",
        "    u\"(*￣m￣)\": \"emoneg\",\n",
        "}\n",
        "\n",
        "for emote, val in EMOTICONS.items():\n",
        "  EMOTICONS[emote] = val.lower().replace(',', ' ').replace(' ', '_')"
      ],
      "metadata": {
        "id": "Rf2Q8b2TbAsr",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_emoticons(text):\n",
        "  return EMOTICONS.get(text, text)\n",
        "  \n",
        "text = \"Hello :-) :-)\"\n",
        "text_split = text.split()\n",
        "for i, txt in enumerate(text_split):\n",
        "  text_split[i] = convert_emoticons(txt)\n",
        "print(' '.join(text_split))"
      ],
      "metadata": {
        "id": "x-jJdrc-bpp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contractions"
      ],
      "metadata": {
        "id": "h9wPsJ3ghEBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Contraction Mapping\n",
        "contraction_mapping = {\n",
        "    \"ain't\": \"is not\", \n",
        "    \"aren't\": \"are not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"'cause\": \"because\", \n",
        "    \"could've\": \"could have\", \n",
        "    \"couldn't\": \"could not\", \n",
        "    \"didn't\": \"did not\",  \n",
        "    \"doesn't\": \"does not\", \n",
        "    \"don't\": \"do not\", \n",
        "    \"hadn't\": \"had not\", \n",
        "    \"hasn't\": \"has not\", \n",
        "    \"haven't\": \"have not\", \n",
        "    \"he'd\": \"he would\",\n",
        "    \"he'll\": \"he will\", \n",
        "    \"he's\": \"he is\", \n",
        "    \"how'd\": \"how did\", \n",
        "    \"how'd'y\": \"how do you\", \n",
        "    \"how'll\": \"how will\", \n",
        "    \"how's\": \"how is\",  \n",
        "    \"I'd\": \"I would\", \n",
        "    \"I'd've\": \"I would have\", \n",
        "    \"I'll\": \"I will\", \n",
        "    \"I'll've\": \"I will have\",\n",
        "    \"I'm\": \"I am\", \n",
        "    \"I've\": \"I have\", \n",
        "    \"i'd\": \"i would\", \n",
        "    \"i'd've\": \"i would have\", \n",
        "    \"i'll\": \"i will\",  \n",
        "    \"i'll've\": \"i will have\",\n",
        "    \"i'm\": \"i am\", \n",
        "    \"i've\": \"i have\", \n",
        "    \"isn't\": \"is not\", \n",
        "    \"it'd\": \"it would\", \n",
        "    \"it'd've\": \"it would have\", \n",
        "    \"it'll\": \"it will\", \n",
        "    \"it'll've\": \"it will have\",\n",
        "    \"it's\": \"it is\", \n",
        "    \"let's\": \"let us\", \n",
        "    \"ma'am\": \"madam\", \n",
        "    \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }"
      ],
      "metadata": {
        "id": "VgcFh25YhG5U",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(contraction_mapping)"
      ],
      "metadata": {
        "id": "Rydaj1_hLuSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_contraction(text): # Before expanding contraction, might want to clean of symbols that are not '\n",
        "  return contraction_mapping.get(text, text)\n",
        "\n",
        "text = \"You're a pig and I should've slayed you, grrr\"\n",
        "text_split = text.split()\n",
        "for i, txt in enumerate(text_split):\n",
        "  text_split[i] = expand_contraction(txt.lower())\n",
        "print(' '.join(text_split))"
      ],
      "metadata": {
        "id": "Sl1TXvlvmjdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Slang/Vocabulary"
      ],
      "metadata": {
        "id": "gt7C3v2wNuNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_mapping = {\n",
        "    'ta': 'teaching assistant',\n",
        "    'biz': 'business',\n",
        "    'hw': 'homework',\n",
        "    'hws': 'homeworks',\n",
        "    'faq': 'frequently answered question',\n",
        "    'faqs': 'frequently answered questions',\n",
        "    'mcq': 'multiple choice question',\n",
        "    'mcqs': 'multiple choice questions',\n",
        "    'frq': 'free response question',\n",
        "    'frqs': 'free response questions',\n",
        "    'ppt': 'powerpoint',\n",
        "    'ppts': 'powerpoints',\n",
        "    'ques': 'question',\n",
        "    'bs': 'bullshit',\n",
        "    'bsing': 'bullshitting',\n",
        "    'bsed': 'bullshitted',\n",
        "    'lol': 'laugh out loud',\n",
        "    'btw': 'by the way',\n",
        "    'imo': 'in my opinion',\n",
        "    'imho': 'in my honest opinion',\n",
        "    'tbh': 'to be honest',\n",
        "    'asap': 'as soon as possible',\n",
        "    'idc': 'i do not care',\n",
        "    'omg': 'oh my god',\n",
        "    'ppl': 'people',\n",
        "    'rip': 'rest in peace',\n",
        "    'srsly': 'seriously',\n",
        "    'thx': 'thanks',\n",
        "    'txt': 'text',\n",
        "    'ur': 'your',\n",
        "    'tho': 'though',\n",
        "    'wtf': 'what the fuck',\n",
        "    'wth': 'what the heck',\n",
        "    'bc': 'because',\n",
        "    'b4': 'before',\n",
        "    'h8': 'hate',\n",
        "    'jk': 'just kidding'\n",
        "}"
      ],
      "metadata": {
        "id": "4c8Slb5mNx8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spellchecker"
      ],
      "metadata": {
        "id": "eWtsBJUGtPK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "# dictionary_path = pkg_resources.resource_filename(\n",
        "#     \"symspellpy\", \"frequency_dictionary_en_82_765.txt\"\n",
        "# )\n",
        "# # term_index is the column of the term and count_index is the\n",
        "# # column of the term frequency\n",
        "# sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "\n",
        "# # lookup suggestions for single-word input strings\n",
        "# input_term = \"memebers\"  # misspelling of \"members\"\n",
        "\n",
        "# # Verbosity.TOP gets the best suggestion\n",
        "# suggestion = sym_spell.lookup(input_term, Verbosity.TOP, max_edit_distance=2)\n",
        "# print(suggestion[0], len(suggestion))"
      ],
      "metadata": {
        "id": "FKD7XvKttROM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopwords"
      ],
      "metadata": {
        "id": "9q3TYcnvmjP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# amend list of stop words to keep whatever it is we want by removing words from list that we want to keep\n",
        "\n",
        "# TODO: is the list of stopwords on git complete and accurate or does someone want to read through all 325 stopwords spacy gives and determine which ones to keep?\n",
        "# stopwords = STOP_WORDS\n",
        "# stopwords.remove(\"but\")\n",
        "# stopwords.remove(\"not\")\n",
        "# stopwords.remove(\"nor\")\n",
        "# stopwords.remove(\"never\")\n",
        "gen_stops = set([\"mr\", \"ms\", \"dr\", \"doctor\", \"s\", \"t\", \"i\", \"me\", \"myself\", \"is\", \"she\", \"he\", \"we\", \"him\", \"her\", \"it\"])\n",
        "domain_stops = set([\"book\", \"books\", \"college\", \"colleges\", \"lecture\", \"lectures\", \"university\", \"universities\", \"lab\", \"labs\", \"hw\", \"hws\", \"quiz\", \"quizzes\", \"prof\", \"professor\", \"teacher\", \"class\", \"classes\", \"course\", \"courses\"])\n",
        "stopwords = gen_stops.union(domain_stops)"
      ],
      "metadata": {
        "id": "kYOoJLdAmkw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spacy Setup"
      ],
      "metadata": {
        "id": "PT1TVLy465BY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm', exclude=['lemmatizer', 'parser', 'textcat', 'custom'])"
      ],
      "metadata": {
        "id": "lyioHaPm67ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Undersampling\n",
        "Currently the method used to undersample is messy.\n",
        "Not only this, but with multinomial nb the stats are:"
      ],
      "metadata": {
        "id": "zOf9dOyURHNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reviews_pos = reviews[reviews['sentiment'] == 1]\n",
        "# reviews_neg = reviews[reviews['sentiment'] == 0]\n",
        "# print(len(reviews_pos), len(reviews_neg))\n",
        "# reviews_pos = reviews[reviews['sentiment'] == 1].sample(n = len(reviews_neg), random_state=1) # Messy way of undersampling\n",
        "\n",
        "# print(len(reviews_pos))\n",
        "\n",
        "# reviews_pos.reset_index(inplace=True, drop=True)\n",
        "# reviews_neg.reset_index(inplace=True, drop=True)\n",
        "# reviews = pd.concat([reviews_pos, reviews_neg], ignore_index=True)\n",
        "\n",
        "# print(reviews.sentiment.value_counts())"
      ],
      "metadata": {
        "id": "n2-48HYpRGsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Pipeline"
      ],
      "metadata": {
        "id": "x8c1-4PfqaCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spellchecked_comments = []\n",
        "lemm = WordNetLemmatizer()\n",
        "grades = set(['a', 'b', 'c', 'd', 'e', 'f'])\n",
        "unseen = Counter()\n",
        "\n",
        "def preprocess_pipeline(df):\n",
        "  cnt = 0 # to keep track of progress\n",
        "  comments_proper = []\n",
        "  for index, review in df.iterrows():\n",
        "    comment = review['comment']\n",
        "    fname = review['firstName'].lower().split(' ')\n",
        "    lname = review['lastName'].lower().split(' ')\n",
        "    names = set(fname + lname)\n",
        "\n",
        "    cnt += 1\n",
        "    if cnt % 25000 == 0:\n",
        "      print(cnt)\n",
        "\n",
        "    comment = remove_urls(comment)\n",
        "    comment = remove_phones(comment)\n",
        "    comment = remove_emails(comment)\n",
        "    comment = remove_html_entities(comment)\n",
        "    comment = remove_html_tags(comment)\n",
        "\n",
        "    comment_split = comment.split(' ')\n",
        "    new_comment_split = []\n",
        "    for i, word in enumerate(comment_split):\n",
        "      word = convert_emoticons(word)\n",
        "      word = word.lower()\n",
        "      word = expand_contraction(word)\n",
        "      word = re.sub(\"[^a-z\\s]+\", ' ', word)   # replace characters that are not alphabetic, space, or underscore\n",
        "      # word = word.replace(\"'\", ' ') # replace apostrophe with space\n",
        "      word = re.sub(r'(.)\\1\\1+', '\\g<1>', word)  # replace any three character+ sequence with one\n",
        "      word = re.sub('\\s+', ' ', word)\n",
        "      word = word.strip() # trailing whitespace because punctuation replaced by space\n",
        "      # if word not in names:\n",
        "      new_comment_split.extend(word.split(' '))\n",
        "\n",
        "    # comment = comment.lower()\n",
        "    # comment = re.sub(\"[^a-zA-Z\\s]+\", ' ', comment)   # replace characters that are not alphabetic, space, or underscore\n",
        "    # comment = comment.replace(\"'\", '') # remove apostrophes\n",
        "    # comment = re.sub(r'(.)\\1\\1+', '\\g<1>', comment)  # replace any three characters sequence with one\n",
        "    # comment = re.sub('\\s+', ' ', comment)\n",
        "    # comment = comment.strip() # trailing whitespace because punctuation replaced by space\n",
        "\n",
        "    \n",
        "    # comment_split = comment.split(' ')\n",
        "    # new_comment_split = []\n",
        "    # for i, word in enumerate(comment_split):\n",
        "    #   if word not in names:\n",
        "    #     new_comment_split.append(word)\n",
        "\n",
        "    # Remove names from the comment\n",
        "    for i, word in enumerate(new_comment_split):\n",
        "      if word in names:\n",
        "        new_comment_split[i] = ''\n",
        "\n",
        "    comment = ' '.join(new_comment_split)\n",
        "    comment = re.sub('\\s+', ' ', comment)\n",
        "    comment = comment.strip()\n",
        "\n",
        "    # comment = [lemm.lemmatize(word) for word in comment.split()] # Lemmatize\n",
        "    # comment = [word for word in comment.split() if word not in stopwords] # remove stopwords\n",
        "    # comment = \" \".join(comment)\n",
        "\n",
        "    # comment = ' '.join(word for word in comment.split() if len(word) > 1)\n",
        "\n",
        "    comments_proper.append(comment)\n",
        "    # spellchecked_comments.append(' '.join(sym_spell.lookup(word, Verbosity.TOP, max_edit_distance=2, include_unknown=True)[0].term for word in comment.split()))\n",
        "  return comments_proper\n",
        "\n",
        "comments_proper = preprocess_pipeline(reviews)"
      ],
      "metadata": {
        "id": "hI6lvtrgqc0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unseen_words = Counter()\n",
        "people = Counter()\n",
        "def preprocess_pipe(texts):\n",
        "    preproc_pipe = []\n",
        "    for doc in nlp.pipe(texts, batch_size=200):\n",
        "      # for word in doc:\n",
        "      #   if word.pos_ == 'PROPN':\n",
        "      #     unseen_words[word.text] += 1\n",
        "      #     print(word.text, word.pos_)\n",
        "      # print(doc.ents)\n",
        "      for word in doc.ents:\n",
        "        if word.label_ == 'PERSON':\n",
        "          people[word.text] += 1\n",
        "          # print(word.text,word.label_)\n",
        "\n",
        "# preprocess_pipe(comments_proper)\n",
        "# print(unseen_words, len(unseen_words))\n",
        "# print(people, len(people))"
      ],
      "metadata": {
        "id": "3YUgaFJgA4VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(people)"
      ],
      "metadata": {
        "id": "FoPSuw4tS6hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i = 0\n",
        "# for comment, spellcheck_comment in zip(comments_proper, spellchecked_comments):\n",
        "#   print(comment)\n",
        "#   print(spellcheck_comment)\n",
        "#   print('\\n')\n",
        "#   i += 1\n",
        "#   if i == 10:\n",
        "#     break"
      ],
      "metadata": {
        "id": "JEQYPIoJsS7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews[\"cleanedComment\"] = pd.Series(comments_proper)\n",
        "# reviews[\"cleanedCommentChecked\"] = pd.Series(spellchecked_comments)\n",
        "reviews['cleanedComment'].head(25)"
      ],
      "metadata": {
        "id": "xOD02O1J8tGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews['sentiment'].value_counts()"
      ],
      "metadata": {
        "id": "lnRBko2cixUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in reviews.head(50).iterrows():\n",
        "    print(row['comment'])\n",
        "    print(row['cleanedComment'])\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "GHyYnLveFXXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping rows <= 5\n",
        "# reviews['wordCount'] = reviews[\"cleanedComment\"].str.split().str.len()\n",
        "# reviews[['wordCount', 'cleanedComment']].head(5)\n",
        "\n",
        "# reviews = reviews[reviews['wordCount'] > 5]\n",
        "# reviews.shape\n",
        "reviews = reviews.loc[:, [\"firstName\", \"lastName\", \"comment\", \"cleanedComment\", \"clarityRating\", \"sentiment\", \"professor_id\"]]"
      ],
      "metadata": {
        "id": "sWKakPo-v8xK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document Sentiment Pipeline"
      ],
      "metadata": {
        "id": "2fgpG8vr9bdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedGroupKFold\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "from sklearn.feature_selection import chi2, SelectPercentile, SelectKBest\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "id": "hODBzDigiKcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evalPerformance(yp, yt, mode=\"weighted\"):\n",
        "    prec_score = precision_score(yt, yp)\n",
        "    rec_score = recall_score(yt, yp)\n",
        "    f1 = f1_score(yt, yp, average=mode)\n",
        "    acc_score = accuracy_score(yt, yp)\n",
        "    conf_m = confusion_matrix(yt, yp)\n",
        "\n",
        "\n",
        "    print(f\"Precision Score: {prec_score*100}\")\n",
        "    print(f\"Recall Score: {rec_score*100}\")\n",
        "    print(\"F1 Score: {0}\".format(f1 * 100))\n",
        "    print(\"Accuracy Score: \" + str(acc_score * 100))\n",
        "    print(conf_m)\n"
      ],
      "metadata": {
        "id": "RWK7q8fLik7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_ids = reviews.groupby(['professor_id', 'cleanedComment']) \n",
        "review_ids.first()"
      ],
      "metadata": {
        "id": "LgV4O0LccPE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groups_professor_id_list = np.array(reviews['professor_id'].values)\n",
        "print(groups_professor_id_list[:5])\n",
        "\n",
        "y = reviews['sentiment']\n",
        "print(y.head(5))\n",
        "\n",
        "X = reviews['cleanedComment']\n",
        "X.head(5)"
      ],
      "metadata": {
        "id": "f-Z9aUrLZXEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline"
      ],
      "metadata": {
        "id": "vGK0_PdnY06y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent_pipeline = Pipeline([\n",
        "    # (\"vectorizer\", CountVectorizer(ngram_range=(1,2), max_df=0.5)), \n",
        "    (\"vectorizer\", TfidfVectorizer(stop_words=stopwords)),\n",
        "    (\"selector\"  , SelectPercentile(score_func=chi2)),\n",
        "    (\"classifer\" , MultinomialNB())\n",
        "    # (\"classifer\" , DecisionTreeClassifier(max_depth=5))\n",
        "])"
      ],
      "metadata": {
        "id": "neJl7wH8iFOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_list = {\n",
        "    'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
        "    'vectorizer__min_df': [3, 5, 7, 8, 9, 11, 13],\n",
        "    'selector__percentile': [30, 35, 40, 45, 50, 55, 60, 65, 70],\n",
        "    'classifier__alpha': np.arange(1, 3, .25),\n",
        "}"
      ],
      "metadata": {
        "id": "4Xkfrjo7Zyhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sgkf = StratifiedGroupKFold(n_splits = 5)\n",
        "random_search = RandomizedSearchCV(sent_pipeline, param_list, scoring='f1', cv=sgkf, n_iter=20, verbose=3, random_state=1)"
      ],
      "metadata": {
        "id": "GZApmZcFYg_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross Validation"
      ],
      "metadata": {
        "id": "Ut1td3pakWqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_search.fit(X, y, groups=groups_professor_id_list)"
      ],
      "metadata": {
        "id": "D5B4B3KDZRcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy"
      ],
      "metadata": {
        "id": "K4xgujDtOAnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(random_search.best_params_)\n",
        "print(random_search.best_estimator_)\n",
        "best_estimator = random_search.best_estimator_"
      ],
      "metadata": {
        "id": "bs2o0LnM9pN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best_estimator = Pipeline([\n",
        "#     (\"vectorizer\", TfidfVectorizer(ngram_range=(1,2), min_df=8, stop_words=stopwords)),\n",
        "#     (\"selector\"  , SelectPercentile(score_func=chi2, percentile=30)),\n",
        "#     (\"classifer\" , MultinomialNB())\n",
        "# ])\n",
        "# best_estimator.fit(X, y)"
      ],
      "metadata": {
        "id": "v7SIxo7XlYwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validating (or Testing???)"
      ],
      "metadata": {
        "id": "aJURPM6lkxvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_reviews = pd.read_csv(\"/content/drive/MyDrive/RMP/scraped_comments.csv\").sample(n=120000, random_state=1)"
      ],
      "metadata": {
        "id": "grbfpSHzj4vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape before dropping:\", test_reviews.shape)\n",
        "test_reviews.drop_duplicates(subset=\"comment_id\", keep=\"first\", inplace=True)\n",
        "\n",
        "# drop rows containing only \"No Comments\" (default value assigned by RMP to a review that didn't enter a comment)\n",
        "test_reviews = test_reviews[test_reviews[\"comment\"] != \"No Comments\"]\n",
        "\n",
        "# drop rows containing NaN comment\n",
        "test_reviews.dropna(subset=[\"comment\"], inplace=True)\n",
        "\n",
        "# fill null names with empty string\n",
        "test_reviews['firstName'].fillna('', inplace=True)\n",
        "test_reviews['lastName'].fillna('', inplace=True)\n",
        "\n",
        "# Dropping test_reviews with clarityRating == 3\n",
        "test_reviews = test_reviews[test_reviews[\"clarityRating\"] != 3]\n",
        "test_reviews[\"sentiment\"] = test_reviews[\"clarityRating\"].apply(lambda x: 1 if x > 2.5 else 0)\n",
        "\n",
        "print(\"Shape after dropping:\", test_reviews.shape)\n",
        "test_reviews.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "DNKBABCDkInt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comments_proper = []\n",
        "\n",
        "comments_proper = preprocess_pipeline(test_reviews)"
      ],
      "metadata": {
        "id": "89kZU5EskZst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_predA = best_estimator.predict(comments_proper)"
      ],
      "metadata": {
        "id": "sL639IqRkpVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_reviews.reset_index(inplace=True, drop=True)"
      ],
      "metadata": {
        "id": "JtYNIKMUmzyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy"
      ],
      "metadata": {
        "id": "XlGQQhFDafI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evalPerformance(sent_predA, test_reviews['sentiment'])\n",
        "'''\n",
        "20% + 5 min_df\n",
        "Precision Score: 89.49715296014145\n",
        "Recall Score: 97.27572028090601\n",
        "F1 Score: 89.66183780505376\n",
        "Accuracy Score: 90.03177638984351\n",
        "\n",
        "25%\n",
        "Precision Score: 88.8488884888849\n",
        "Recall Score: 97.6445154088539\n",
        "F1 Score: 89.25215219146043\n",
        "Accuracy Score: 89.70006674037992\n",
        "\n",
        "40%\n",
        "Precision Score: 88.47752413837202\n",
        "Recall Score: 97.75896906925153\n",
        "F1 Score: 88.95187701191335\n",
        "Accuracy Score: 89.44505872157308\n",
        "'''\n",
        "# 93.479 and 93.4242%; min_df = 1\n",
        "# 92.55 and 92.66; min_df = 6\n",
        "# 92.4659 F1 for min_df = 15"
      ],
      "metadata": {
        "id": "c-q4csk0k8D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "right = sum(a == b for a, b in zip(sent_predA, test_reviews['sentiment']))\n",
        "right / len(sent_predA)"
      ],
      "metadata": {
        "id": "amqFgxlce_bB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Possible Improvements\n",
        "* Could engineer new features using words that are capitalized in the review\n",
        "* Remove reviews that're not english"
      ],
      "metadata": {
        "id": "Jzl-f7BvdTIS"
      }
    }
  ]
}